{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaia CV Hunter v2: Scaled Anomaly Detection Pipeline\n",
    "\n",
    "**Goal:** Find rare cataclysmic variables (CVs), especially short-period dwarf novae, using ML anomaly detection on Gaia DR3 variability statistics.\n",
    "\n",
    "**Key features:**\n",
    "- Larger sample size (2000-5000 sources) with smart blue/faint filtering\n",
    "- Dual ML models: Isolation Forest + One-Class SVM\n",
    "- **Top 10+ anomaly analysis** with automated TESS light curve extraction\n",
    "- **Automated multi-survey cross-matching:**\n",
    "  - VSX (Variable Star Index) - known variable star classifications\n",
    "  - ROSAT - X-ray detections (accretion indicator)\n",
    "  - TESS - high-cadence light curves\n",
    "  - SIMBAD - existing astronomical classifications\n",
    "  - Gaia DR3 `vari_classifier_result` - automated variability classes\n",
    "  - ZTF - dense optical light curves for short-period systems\n",
    "  - GALEX - UV excess from hot accretion disks\n",
    "- Prioritized candidate output with multi-wavelength evidence scoring\n",
    "\n",
    "**Author:** Landon Mutch\n",
    "**Repository:** https://github.com/toadlyBroodle/science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install required packages\n",
    "!pip install astroquery astropy scikit-learn pandas matplotlib seaborn --quiet\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports and configuration\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Astropy/Astroquery\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from astroquery.gaia import Gaia\n",
    "from astroquery.vizier import Vizier\n",
    "from astroquery.simbad import Simbad as SimbadClass\n",
    "from astroquery.mast import Tesscut, Catalogs\n",
    "\n",
    "# ML\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'sample_size': 5000,           # Target number of sources\n",
    "    'bp_rp_max': 0.8,              # Blue color cut (CVs are blue)\n",
    "    'g_mag_min': 14.0,             # Faint sources (avoid saturated)\n",
    "    'g_mag_max': 18.5,             # Not too faint (need good S/N)\n",
    "    'anomaly_percentile': 2,       # Top N% as anomalies\n",
    "    # Quantitative thresholds (score-based, not count-based)\n",
    "    'anomaly_score_min': 0.4,      # Min combined anomaly score [0-1] to cross-match\n",
    "    'require_consensus': True,      # Require BOTH models to flag as anomaly\n",
    "    'priority_score_min': 10.0,    # Min priority score for TESS LC extraction & deep investigation\n",
    "    # Search radii\n",
    "    'tess_search_radius': 21,      # arcsec (TESS pixel = 21\")\n",
    "    'rosat_search_radius': 30,     # arcsec\n",
    "    'simbad_search_radius': 5,     # arcsec\n",
    "    'ztf_search_radius': 3,        # arcsec\n",
    "    'galex_search_radius': 5,      # arcsec\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "print(\"\\nImports complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Query Gaia DR3 with Smart Filtering\n",
    "\n",
    "Target: Blue, faint variables with good variability statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Query Gaia DR3 for CV candidates\n",
    "print(\"=\"*70)\n",
    "print(\"QUERYING GAIA DR3 FOR VARIABLE STAR CANDIDATES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Query focuses on:\n",
    "# - Blue sources (BP-RP < 0.8) where CVs and WDs live\n",
    "# - Moderate magnitudes (14 < G < 18.5) for good S/N\n",
    "# - Sources with variability statistics\n",
    "# - High amplitude or unusual skewness/kurtosis\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT TOP {CONFIG['sample_size']}\n",
    "    vs.source_id,\n",
    "    gs.ra, gs.dec,\n",
    "    gs.phot_g_mean_mag,\n",
    "    gs.bp_rp,\n",
    "    gs.parallax,\n",
    "    gs.parallax_error,\n",
    "    gs.pmra, gs.pmdec,\n",
    "    vs.mean_mag_g_fov,\n",
    "    vs.std_dev_mag_g_fov,\n",
    "    vs.range_mag_g_fov,\n",
    "    vs.skewness_mag_g_fov,\n",
    "    vs.kurtosis_mag_g_fov,\n",
    "    vs.num_selected_g_fov,\n",
    "    vs.mean_obs_time_g_fov,\n",
    "    vs.time_duration_g_fov\n",
    "FROM gaiadr3.vari_summary AS vs\n",
    "JOIN gaiadr3.gaia_source AS gs ON vs.source_id = gs.source_id\n",
    "WHERE gs.bp_rp IS NOT NULL\n",
    "    AND gs.bp_rp < {CONFIG['bp_rp_max']}\n",
    "    AND gs.phot_g_mean_mag > {CONFIG['g_mag_min']}\n",
    "    AND gs.phot_g_mean_mag < {CONFIG['g_mag_max']}\n",
    "    AND vs.std_dev_mag_g_fov > 0.05\n",
    "    AND vs.num_selected_g_fov > 10\n",
    "    AND gs.parallax IS NOT NULL\n",
    "    AND gs.parallax > 0.1\n",
    "ORDER BY vs.range_mag_g_fov DESC, vs.source_id ASC\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nQuery parameters:\")\n",
    "print(f\"  Max sample: {CONFIG['sample_size']} (deterministic by range_mag desc, source_id)\")\n",
    "print(f\"  Color cut: BP-RP < {CONFIG['bp_rp_max']} (blue sources)\")\n",
    "print(f\"  Magnitude range: {CONFIG['g_mag_min']} < G < {CONFIG['g_mag_max']}\")\n",
    "print(f\"  Variability: std_dev > 0.05 mag, >10 observations\")\n",
    "\n",
    "print(\"\\nSubmitting async query to Gaia Archive...\")\n",
    "print(\"(This may take 1-3 minutes)\")\n",
    "\n",
    "try:\n",
    "    job = Gaia.launch_job_async(query)\n",
    "    results = job.get_results()\n",
    "    df = results.to_pandas()\n",
    "    n_total = len(df)\n",
    "    print(f\"\\n✓ Retrieved {n_total} sources from Gaia DR3\")\n",
    "    \n",
    "    # Deterministic truncation if needed (already ordered by range DESC, source_id ASC)\n",
    "    if CONFIG['sample_size'] > 0 and n_total > CONFIG['sample_size']:\n",
    "        df = df.head(CONFIG['sample_size'])\n",
    "        print(f\"  Truncated to top {CONFIG['sample_size']} by variability range (deterministic)\")\n",
    "    print(f\"  Working sample: {len(df)} sources\")\n",
    "    \n",
    "    # Quick stats\n",
    "    print(f\"\\nData summary:\")\n",
    "    print(f\"  G magnitude: {df['phot_g_mean_mag'].min():.1f} - {df['phot_g_mean_mag'].max():.1f}\")\n",
    "    print(f\"  BP-RP color: {df['bp_rp'].min():.2f} - {df['bp_rp'].max():.2f}\")\n",
    "    print(f\"  Amplitude range: {df['range_mag_g_fov'].min():.2f} - {df['range_mag_g_fov'].max():.2f} mag\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(\"\\nTry reducing sample_size or running again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Feature engineering for anomaly detection\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate derived features\n",
    "df['abs_mag_g'] = df['phot_g_mean_mag'] + 5 * np.log10(df['parallax'] / 100)\n",
    "df['amplitude'] = df['range_mag_g_fov']\n",
    "df['rel_std'] = df['std_dev_mag_g_fov'] / df['mean_mag_g_fov'].abs()\n",
    "df['proper_motion'] = np.sqrt(df['pmra']**2 + df['pmdec']**2)\n",
    "df['distance_pc'] = 1000 / df['parallax']\n",
    "\n",
    "# Features for ML\n",
    "feature_cols = [\n",
    "    'amplitude',\n",
    "    'std_dev_mag_g_fov',\n",
    "    'skewness_mag_g_fov',\n",
    "    'kurtosis_mag_g_fov',\n",
    "    'bp_rp',\n",
    "    'abs_mag_g',\n",
    "    'rel_std',\n",
    "    'proper_motion'\n",
    "]\n",
    "\n",
    "# Remove rows with NaN in features\n",
    "df_clean = df.dropna(subset=feature_cols).copy()\n",
    "print(f\"\\nSources after cleaning: {len(df_clean)} (dropped {len(df) - len(df_clean)} with NaN)\")\n",
    "\n",
    "# Prepare feature matrix\n",
    "X = df_clean[feature_cols].values\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\nFeatures used ({len(feature_cols)}):\")\n",
    "for i, col in enumerate(feature_cols):\n",
    "    print(f\"  {i+1}. {col}: mean={X[:, i].mean():.3f}, std={X[:, i].std():.3f}\")\n",
    "\n",
    "print(\"\\n✓ Features prepared and scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Dual ML Anomaly Detection\n",
    "\n",
    "Using both Isolation Forest and One-Class SVM for robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Dual ML anomaly detection\n",
    "print(\"=\"*70)\n",
    "print(\"ANOMALY DETECTION: ISOLATION FOREST + ONE-CLASS SVM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "contamination = CONFIG['anomaly_percentile'] / 100\n",
    "\n",
    "# Model 1: Isolation Forest\n",
    "print(\"\\n1. Training Isolation Forest...\")\n",
    "iso_forest = IsolationForest(\n",
    "    contamination=contamination,\n",
    "    random_state=42,\n",
    "    n_estimators=200,\n",
    "    max_samples='auto'\n",
    ")\n",
    "iso_labels = iso_forest.fit_predict(X_scaled)\n",
    "iso_scores = -iso_forest.score_samples(X_scaled)  # Higher = more anomalous\n",
    "\n",
    "n_iso_anomalies = (iso_labels == -1).sum()\n",
    "print(f\"   Isolation Forest anomalies: {n_iso_anomalies}\")\n",
    "\n",
    "# Model 2: One-Class SVM\n",
    "print(\"\\n2. Training One-Class SVM...\")\n",
    "oc_svm = OneClassSVM(\n",
    "    nu=contamination,\n",
    "    kernel='rbf',\n",
    "    gamma='scale'\n",
    ")\n",
    "svm_labels = oc_svm.fit_predict(X_scaled)\n",
    "svm_scores = -oc_svm.score_samples(X_scaled)  # Higher = more anomalous\n",
    "\n",
    "n_svm_anomalies = (svm_labels == -1).sum()\n",
    "print(f\"   One-Class SVM anomalies: {n_svm_anomalies}\")\n",
    "\n",
    "# Combined scoring\n",
    "# Normalize scores to [0, 1] and average\n",
    "iso_norm = (iso_scores - iso_scores.min()) / (iso_scores.max() - iso_scores.min())\n",
    "svm_norm = (svm_scores - svm_scores.min()) / (svm_scores.max() - svm_scores.min())\n",
    "combined_score = (iso_norm + svm_norm) / 2\n",
    "\n",
    "# Add scores to dataframe\n",
    "df_clean['iso_score'] = iso_scores\n",
    "df_clean['svm_score'] = svm_scores\n",
    "df_clean['combined_score'] = combined_score\n",
    "df_clean['iso_anomaly'] = iso_labels == -1\n",
    "df_clean['svm_anomaly'] = svm_labels == -1\n",
    "df_clean['both_anomaly'] = (iso_labels == -1) & (svm_labels == -1)\n",
    "\n",
    "n_both = df_clean['both_anomaly'].sum()\n",
    "print(f\"\\n3. Consensus anomalies (flagged by BOTH models): {n_both}\")\n",
    "\n",
    "# Rank by combined score\n",
    "df_clean = df_clean.sort_values('combined_score', ascending=False)\n",
    "\n",
    "print(\"\\n✓ Anomaly detection complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Visualize Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Visualize anomalies\n",
    "print(\"=\"*70)\n",
    "print(\"ANOMALY VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Select candidates by quantitative threshold, not fixed count\n",
    "score_threshold = CONFIG['anomaly_score_min']\n",
    "if CONFIG['require_consensus']:\n",
    "    top_candidates = df_clean[\n",
    "        (df_clean['combined_score'] >= score_threshold) & df_clean['both_anomaly']\n",
    "    ].copy()\n",
    "else:\n",
    "    top_candidates = df_clean[\n",
    "        df_clean['combined_score'] >= score_threshold\n",
    "    ].copy()\n",
    "\n",
    "n_selected = len(top_candidates)\n",
    "print(f\"\\nCandidates selected: {n_selected}\")\n",
    "print(f\"  Score threshold: combined_score >= {score_threshold}\")\n",
    "print(f\"  Consensus required: {CONFIG['require_consensus']}\")\n",
    "if n_selected > 0:\n",
    "    print(f\"  Score range: {top_candidates['combined_score'].min():.3f} - {top_candidates['combined_score'].max():.3f}\")\n",
    "\n",
    "# 1. Color-Magnitude Diagram\n",
    "ax1 = axes[0, 0]\n",
    "ax1.scatter(df_clean['bp_rp'], df_clean['abs_mag_g'],\n",
    "            c=df_clean['combined_score'], cmap='viridis',\n",
    "            s=5, alpha=0.5)\n",
    "ax1.scatter(top_candidates['bp_rp'], top_candidates['abs_mag_g'],\n",
    "            c='red', s=50, marker='*', label=f'{n_selected} anomalies (score>={score_threshold})', zorder=10)\n",
    "ax1.invert_yaxis()\n",
    "ax1.set_xlabel('BP-RP Color')\n",
    "ax1.set_ylabel('Absolute G Magnitude')\n",
    "ax1.set_title('Color-Magnitude Diagram')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Amplitude vs Skewness\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(df_clean['amplitude'], df_clean['skewness_mag_g_fov'],\n",
    "            c=df_clean['combined_score'], cmap='viridis',\n",
    "            s=5, alpha=0.5)\n",
    "ax2.scatter(top_candidates['amplitude'], top_candidates['skewness_mag_g_fov'],\n",
    "            c='red', s=50, marker='*', zorder=10)\n",
    "ax2.set_xlabel('Amplitude (mag)')\n",
    "ax2.set_ylabel('Skewness')\n",
    "ax2.set_title('Amplitude vs Skewness')\n",
    "ax2.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 3. Skewness vs Kurtosis\n",
    "ax3 = axes[1, 0]\n",
    "ax3.scatter(df_clean['skewness_mag_g_fov'], df_clean['kurtosis_mag_g_fov'],\n",
    "            c=df_clean['combined_score'], cmap='viridis',\n",
    "            s=5, alpha=0.5)\n",
    "ax3.scatter(top_candidates['skewness_mag_g_fov'], top_candidates['kurtosis_mag_g_fov'],\n",
    "            c='red', s=50, marker='*', zorder=10)\n",
    "ax3.set_xlabel('Skewness')\n",
    "ax3.set_ylabel('Kurtosis')\n",
    "ax3.set_title('Light Curve Shape: Skewness vs Kurtosis')\n",
    "ax3.axhline(3, color='gray', linestyle='--', alpha=0.5, label='Gaussian')\n",
    "ax3.axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 4. Score distribution with threshold\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(df_clean['combined_score'], bins=50, alpha=0.7, label='All sources')\n",
    "ax4.axvline(score_threshold, color='red', linestyle='--', lw=2,\n",
    "            label=f'Threshold = {score_threshold} ({n_selected} selected)')\n",
    "# Also show consensus-only count\n",
    "n_consensus = df_clean['both_anomaly'].sum()\n",
    "ax4.axvline(df_clean[df_clean['both_anomaly']]['combined_score'].min(), color='orange',\n",
    "            linestyle=':', lw=1.5, label=f'Both-model consensus ({n_consensus} total)')\n",
    "ax4.set_xlabel('Combined Anomaly Score')\n",
    "ax4.set_ylabel('Count')\n",
    "ax4.set_title('Anomaly Score Distribution')\n",
    "ax4.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('anomaly_detection_v2.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Saved to anomaly_detection_v2.png\")\n",
    "print(f\"  {n_selected} candidates passed threshold (vs fixed top-N approach)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Cross-match with VSX (Variable Star Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Cross-match top candidates with VSX\n",
    "print(\"=\"*70)\n",
    "print(\"CROSS-MATCHING WITH VSX (Variable Star Index)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# top_candidates already defined by score threshold in Cell 6\n",
    "print(f\"\\nChecking {len(top_candidates)} anomaly candidates against VSX...\")\n",
    "print(f\"  (selected by combined_score >= {CONFIG['anomaly_score_min']}, consensus={CONFIG['require_consensus']})\")\n",
    "\n",
    "# VSX catalog in VizieR\n",
    "vsx_catalog = 'B/vsx/vsx'\n",
    "Vizier.ROW_LIMIT = 5\n",
    "\n",
    "vsx_names = []\n",
    "vsx_types = []\n",
    "vsx_periods = []\n",
    "in_vsx_flags = []\n",
    "\n",
    "for i, (idx, row) in enumerate(top_candidates.iterrows()):\n",
    "    coord = SkyCoord(ra=row['ra']*u.deg, dec=row['dec']*u.deg, frame='icrs')\n",
    "    if (i + 1) % 10 == 0 or i == 0 or i + 1 == len(top_candidates):\n",
    "        print(f\"  [{i+1}/{len(top_candidates)}]...\", flush=True)\n",
    "\n",
    "    try:\n",
    "        result = Vizier.query_region(coord, radius=5*u.arcsec, catalog=vsx_catalog)\n",
    "\n",
    "        if result and len(result) > 0 and len(result[0]) > 0:\n",
    "            vsx = result[0][0]\n",
    "            vsx_names.append(vsx['Name'] if 'Name' in vsx.colnames else 'Unknown')\n",
    "            vsx_types.append(vsx['Type'] if 'Type' in vsx.colnames else 'VAR')\n",
    "            vsx_periods.append(float(vsx['Period']) if 'Period' in vsx.colnames and vsx['Period'] else None)\n",
    "            in_vsx_flags.append(True)\n",
    "        else:\n",
    "            vsx_names.append(None)\n",
    "            vsx_types.append(None)\n",
    "            vsx_periods.append(None)\n",
    "            in_vsx_flags.append(False)\n",
    "    except Exception as e:\n",
    "        vsx_names.append(None)\n",
    "        vsx_types.append(None)\n",
    "        vsx_periods.append(None)\n",
    "        in_vsx_flags.append(False)\n",
    "\n",
    "# Assign directly\n",
    "top_candidates['vsx_name'] = vsx_names\n",
    "top_candidates['vsx_type'] = vsx_types\n",
    "top_candidates['vsx_period'] = vsx_periods\n",
    "top_candidates['in_vsx'] = in_vsx_flags\n",
    "\n",
    "n_in_vsx = top_candidates['in_vsx'].sum()\n",
    "n_not_in_vsx = len(top_candidates) - n_in_vsx\n",
    "print(f\"\\n✓ VSX cross-match complete:\")\n",
    "print(f\"   In VSX: {n_in_vsx}\")\n",
    "print(f\"   NOT in VSX (potentially uncatalogued): {n_not_in_vsx}\")\n",
    "\n",
    "# Show VSX types found\n",
    "if n_in_vsx > 0:\n",
    "    print(f\"\\nVSX types found:\")\n",
    "    type_counts = top_candidates[top_candidates['in_vsx']]['vsx_type'].value_counts()\n",
    "    for vtype, count in type_counts.items():\n",
    "        print(f\"   {vtype}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Cross-match with ROSAT (X-ray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Cross-match with ROSAT X-ray catalog\n",
    "print(\"=\"*70)\n",
    "print(\"CROSS-MATCHING WITH ROSAT (X-ray Survey)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nChecking {len(top_candidates)} candidates for X-ray counterparts...\")\n",
    "print(\"(X-ray detection is strong evidence for CVs)\")\n",
    "\n",
    "# ROSAT catalogs\n",
    "rosat_catalogs = [\n",
    "    'IX/10A/1rxs',    # ROSAT All-Sky Survey Bright Source Catalog\n",
    "    'IX/29/rass_fsc'  # ROSAT All-Sky Survey Faint Source Catalog\n",
    "]\n",
    "\n",
    "has_xray_flags = []\n",
    "xray_rates = []\n",
    "\n",
    "for i, (idx, row) in enumerate(top_candidates.iterrows()):\n",
    "    coord = SkyCoord(ra=row['ra']*u.deg, dec=row['dec']*u.deg, frame='icrs')\n",
    "    found_xray = False\n",
    "    xray_flux = None\n",
    "\n",
    "    for cat in rosat_catalogs:\n",
    "        try:\n",
    "            result = Vizier.query_region(coord, radius=CONFIG['rosat_search_radius']*u.arcsec, catalog=cat)\n",
    "            if result and len(result) > 0 and len(result[0]) > 0:\n",
    "                found_xray = True\n",
    "                if 'Count' in result[0].colnames:\n",
    "                    xray_flux = float(result[0][0]['Count'])\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    has_xray_flags.append(found_xray)\n",
    "    xray_rates.append(xray_flux)\n",
    "    if (i + 1) % 10 == 0 or i + 1 == len(top_candidates):\n",
    "        print(f\"  [{i+1}/{len(top_candidates)}]...\", flush=True)\n",
    "\n",
    "# Assign directly\n",
    "top_candidates['has_xray'] = has_xray_flags\n",
    "top_candidates['xray_count_rate'] = xray_rates\n",
    "\n",
    "n_xray = top_candidates['has_xray'].sum()\n",
    "print(f\"\\n✓ ROSAT cross-match complete:\")\n",
    "print(f\"   X-ray detections: {n_xray} / {len(top_candidates)}\")\n",
    "\n",
    "if n_xray > 0:\n",
    "    print(f\"\\n   ⭐ X-ray sources are HIGH PRIORITY CV candidates!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Check TESS Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Check TESS coverage for top candidates\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHECKING TESS COVERAGE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_cands = len(top_candidates)\n",
    "print(f\"\\nQuerying TIC for {n_cands} candidates...\")\n",
    "\n",
    "BATCH = 5\n",
    "\n",
    "def _query_tic(args):\n",
    "    idx, ra, dec = args\n",
    "    try:\n",
    "        coord = SkyCoord(ra=ra*u.deg, dec=dec*u.deg, frame=\"icrs\")\n",
    "        tic_result = Catalogs.query_region(coord, radius=CONFIG[\"tess_search_radius\"]*u.arcsec, catalog=\"TIC\")\n",
    "        if tic_result and len(tic_result) > 0:\n",
    "            tic = tic_result[0]\n",
    "            teff = float(tic[\"Teff\"]) if tic[\"Teff\"] and not np.ma.is_masked(tic[\"Teff\"]) else None\n",
    "            rad = float(tic[\"rad\"]) if tic[\"rad\"] and not np.ma.is_masked(tic[\"rad\"]) else None\n",
    "            return idx, int(tic[\"ID\"]), teff, rad\n",
    "    except:\n",
    "        pass\n",
    "    return idx, None, None, None\n",
    "\n",
    "tic_args = [(idx, row[\"ra\"], row[\"dec\"]) for idx, row in top_candidates.iterrows()]\n",
    "tic_map = {}\n",
    "done = 0\n",
    "\n",
    "for b0 in range(0, len(tic_args), BATCH):\n",
    "    batch = tic_args[b0:b0 + BATCH]\n",
    "    with ThreadPoolExecutor(max_workers=BATCH) as pool:\n",
    "        futs = {pool.submit(_query_tic, a): a[0] for a in batch}\n",
    "        for f in as_completed(futs, timeout=60):\n",
    "            try:\n",
    "                idx, tic_id, teff, rad = f.result(timeout=30)\n",
    "                if tic_id is not None:\n",
    "                    tic_map[idx] = {\"tic_id\": tic_id, \"teff\": teff, \"rad\": rad}\n",
    "            except:\n",
    "                pass\n",
    "    done = min(b0 + BATCH, n_cands)\n",
    "    if done % 10 == 0 or done == n_cands:\n",
    "        print(f\"  {done}/{n_cands} ({len(tic_map)} matched)\", flush=True)\n",
    "\n",
    "# Assign results - skip slow Tesscut.get_sectors(), sector info discovered at cutout time\n",
    "tic_ids, tess_teffs, tess_rads = [], [], []\n",
    "for idx in top_candidates.index:\n",
    "    tm = tic_map.get(idx, {})\n",
    "    tic_ids.append(tm.get(\"tic_id\"))\n",
    "    tess_teffs.append(tm.get(\"teff\"))\n",
    "    tess_rads.append(tm.get(\"rad\"))\n",
    "\n",
    "top_candidates[\"tic_id\"] = tic_ids\n",
    "top_candidates[\"tess_teff\"] = tess_teffs\n",
    "top_candidates[\"tess_rad\"] = tess_rads\n",
    "top_candidates[\"n_tess_sectors\"] = [1 if tid else 0 for tid in tic_ids]  # proxy; real count at cutout time\n",
    "top_candidates[\"tess_sectors\"] = [[] for _ in tic_ids]\n",
    "\n",
    "n_with_tic = sum(1 for t in tic_ids if t is not None)\n",
    "print(f\"\\n\\u2713 TIC cross-match complete:\")\n",
    "print(f\"   TIC matches: {n_with_tic} / {n_cands}\")\n",
    "print(f\"   (Sector counts determined during light curve extraction)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Cross-match with SIMBAD\n",
    "\n",
    "Check for any existing astronomical classifications in SIMBAD. Sources NOT in SIMBAD are potentially novel discoveries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Cross-match with SIMBAD for existing classifications\n",
    "print(\"=\"*70)\n",
    "print(\"CROSS-MATCHING WITH SIMBAD\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_cands = len(top_candidates)\n",
    "print(f\"\\nChecking {n_cands} candidates via SIMBAD TAP batch query...\")\n",
    "\n",
    "# --- Approach: single SIMBAD TAP cross-match query ---\n",
    "from astropy.table import Table as AstropyTable\n",
    "import tempfile, os\n",
    "\n",
    "upload_tbl = AstropyTable()\n",
    "upload_tbl[\"source_id\"] = [int(s) for s in top_candidates[\"source_id\"]]\n",
    "upload_tbl[\"ra\"] = top_candidates[\"ra\"].values\n",
    "upload_tbl[\"dec\"] = top_candidates[\"dec\"].values\n",
    "\n",
    "radius_deg = CONFIG[\"simbad_search_radius\"] / 3600.0\n",
    "\n",
    "simbad_tap_query = f\"\"\"\n",
    "SELECT u.source_id, b.main_id, b.otype, b.otypes,\n",
    "       DISTANCE(POINT('ICRS', b.ra, b.dec), POINT('ICRS', u.ra, u.dec)) AS sep\n",
    "FROM TAP_UPLOAD.cands AS u\n",
    "LEFT JOIN basic AS b\n",
    "  ON 1=CONTAINS(POINT('ICRS', b.ra, b.dec), CIRCLE('ICRS', u.ra, u.dec, {radius_deg:.6f}))\n",
    "ORDER BY u.source_id, sep\n",
    "\"\"\"\n",
    "\n",
    "simbad_results = {}\n",
    "tap_success = False\n",
    "\n",
    "try:\n",
    "    from astroquery.simbad import Simbad as SimbadQ\n",
    "    print(\"  Submitting TAP query...\", flush=True)\n",
    "    result = SimbadQ.query_tap(simbad_tap_query, cands=upload_tbl)\n",
    "    if result is not None and len(result) > 0:\n",
    "        tap_success = True\n",
    "        # Take closest match per source_id\n",
    "        seen = set()\n",
    "        for row in result:\n",
    "            sid = int(row[\"source_id\"])\n",
    "            if sid in seen:\n",
    "                continue\n",
    "            seen.add(sid)\n",
    "            mid = str(row[\"main_id\"]).strip() if row[\"main_id\"] and not np.ma.is_masked(row[\"main_id\"]) else None\n",
    "            otype = str(row[\"otype\"]).strip() if row[\"otype\"] and not np.ma.is_masked(row[\"otype\"]) else None\n",
    "            otypes = str(row[\"otypes\"]).strip() if row[\"otypes\"] and not np.ma.is_masked(row[\"otypes\"]) else None\n",
    "            if mid:\n",
    "                simbad_results[sid] = (True, mid, otype, otypes)\n",
    "        print(f\"  TAP returned {len(result)} rows, {len(simbad_results)} unique matches\", flush=True)\n",
    "except Exception as e:\n",
    "    print(f\"  TAP query failed: {e}\", flush=True)\n",
    "\n",
    "# --- Fallback: sequential queries if TAP failed ---\n",
    "if not tap_success:\n",
    "    print(\"  Falling back to sequential queries...\", flush=True)\n",
    "    custom_simbad = SimbadClass()\n",
    "    custom_simbad.add_votable_fields(\"otype\", \"otypes\")\n",
    "    custom_simbad.TIMEOUT = 10\n",
    "\n",
    "    def _simbad_val(table, col_variants):\n",
    "        cols_lower = {c.lower(): c for c in table.colnames}\n",
    "        for name in col_variants:\n",
    "            actual = cols_lower.get(name.lower())\n",
    "            if actual:\n",
    "                val = table[0][actual]\n",
    "                return str(val).strip() if not np.ma.is_masked(val) else None\n",
    "        return None\n",
    "\n",
    "    for i, (idx, row) in enumerate(top_candidates.iterrows()):\n",
    "        sid = int(row[\"source_id\"])\n",
    "        if sid in simbad_results:\n",
    "            continue\n",
    "        try:\n",
    "            result = custom_simbad.query_object(f\"Gaia DR3 {sid}\")\n",
    "            if result is None:\n",
    "                coord = SkyCoord(ra=row[\"ra\"]*u.deg, dec=row[\"dec\"]*u.deg, frame=\"icrs\")\n",
    "                result = custom_simbad.query_region(coord, radius=CONFIG[\"simbad_search_radius\"]*u.arcsec)\n",
    "            if result is not None and len(result) > 0:\n",
    "                name = _simbad_val(result, [\"MAIN_ID\", \"main_id\"])\n",
    "                otype = _simbad_val(result, [\"OTYPE\", \"otype\", \"main_type\"])\n",
    "                otypes = _simbad_val(result, [\"OTYPES\", \"otypes\", \"all_types\"])\n",
    "                if name:\n",
    "                    simbad_results[sid] = (True, name, otype, otypes)\n",
    "        except:\n",
    "            pass\n",
    "        if (i + 1) % 10 == 0 or i + 1 == n_cands:\n",
    "            print(f\"    {i+1}/{n_cands}\", flush=True)\n",
    "\n",
    "# --- Assign results ---\n",
    "simbad_names, simbad_otypes, simbad_all_otypes, in_simbad_flags = [], [], [], []\n",
    "for idx, row in top_candidates.iterrows():\n",
    "    sid = int(row[\"source_id\"])\n",
    "    if sid in simbad_results:\n",
    "        found, name, otype, otypes = simbad_results[sid]\n",
    "        simbad_names.append(name)\n",
    "        simbad_otypes.append(otype)\n",
    "        simbad_all_otypes.append(otypes)\n",
    "        in_simbad_flags.append(True)\n",
    "    else:\n",
    "        simbad_names.append(None)\n",
    "        simbad_otypes.append(None)\n",
    "        simbad_all_otypes.append(None)\n",
    "        in_simbad_flags.append(False)\n",
    "\n",
    "top_candidates[\"simbad_name\"] = simbad_names\n",
    "top_candidates[\"simbad_otype\"] = simbad_otypes\n",
    "top_candidates[\"simbad_otypes\"] = simbad_all_otypes\n",
    "top_candidates[\"in_simbad\"] = in_simbad_flags\n",
    "\n",
    "n_in_simbad = sum(in_simbad_flags)\n",
    "n_not_simbad = n_cands - n_in_simbad\n",
    "\n",
    "print(f\"\\n\\u2713 SIMBAD cross-match complete:\")\n",
    "print(f\"   Found in SIMBAD: {n_in_simbad} / {n_cands}\")\n",
    "print(f\"   NOT in SIMBAD: {n_not_simbad}\")\n",
    "\n",
    "if n_in_simbad > 0:\n",
    "    print(f\"\\nSIMBAD object types:\")\n",
    "    type_counts = top_candidates[top_candidates[\"in_simbad\"]][\"simbad_otype\"].value_counts()\n",
    "    for otype, count in type_counts.items():\n",
    "        print(f\"   {otype}: {count}\")\n",
    "    cv_types = [\"CV\", \"CV*\", \"No*\", \"DN*\", \"DQ*\", \"AM*\", \"NL*\", \"XB*\",\n",
    "                \"CataclyV*\", \"Nova\", \"DwarfNova\", \"AMHer\", \"NovaLike\"]\n",
    "    cv_matches = top_candidates[top_candidates[\"simbad_otype\"].isin(cv_types)]\n",
    "    if len(cv_matches) > 0:\n",
    "        print(f\"\\n   \\u2b50 {len(cv_matches)} sources have CV-related SIMBAD types!\")\n",
    "\n",
    "if n_not_simbad > 0:\n",
    "    print(f\"\\n   \\u2b50 {n_not_simbad} sources NOT in SIMBAD - potentially novel discoveries!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Gaia DR3 Variability Classifier\n",
    "\n",
    "Query `gaiadr3.vari_classifier_result` to check if Gaia's automated pipeline already classified these sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Cross-match with Gaia DR3 variability classifier\n",
    "print(\"=\"*70)\n",
    "print(\"CROSS-MATCHING WITH GAIA DR3 VARIABILITY CLASSIFIER\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nQuerying gaiadr3.vari_classifier_result for {len(top_candidates)} candidates...\")\n",
    "\n",
    "source_ids = top_candidates['source_id'].astype(int).tolist()\n",
    "\n",
    "# Query in batches\n",
    "batch_size = 30\n",
    "vari_lookup = {}  # source_id -> (class, score)\n",
    "\n",
    "for i in range(0, len(source_ids), batch_size):\n",
    "    batch = source_ids[i:i+batch_size]\n",
    "    id_list = ','.join(str(sid) for sid in batch)\n",
    "    print(f\"  Batch {i//batch_size + 1}: querying {len(batch)} sources...\", end=' ', flush=True)\n",
    "    \n",
    "    vari_query = f\"\"\"\n",
    "    SELECT source_id, best_class_name, best_class_score\n",
    "    FROM gaiadr3.vari_classifier_result\n",
    "    WHERE source_id IN ({id_list})\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        job = Gaia.launch_job(vari_query)\n",
    "        result = job.get_results()\n",
    "        if result and len(result) > 0:\n",
    "            for row in result:\n",
    "                vari_lookup[int(row['source_id'])] = (\n",
    "                    str(row['best_class_name']),\n",
    "                    float(row['best_class_score'])\n",
    "                )\n",
    "            print(f\"{len(result)} classified\")\n",
    "        else:\n",
    "            print(\"0 classified\")\n",
    "    except Exception as e:\n",
    "        print(f\"error: {str(e)[:50]}\")\n",
    "\n",
    "# Assign directly to avoid merge issues\n",
    "top_candidates['gaia_vari_class'] = top_candidates['source_id'].apply(\n",
    "    lambda sid: vari_lookup.get(int(sid), (None, None))[0])\n",
    "top_candidates['gaia_vari_score'] = top_candidates['source_id'].apply(\n",
    "    lambda sid: vari_lookup.get(int(sid), (None, None))[1])\n",
    "\n",
    "n_classified = top_candidates['gaia_vari_class'].notna().sum()\n",
    "print(f\"\\n✓ Gaia DR3 variability classifier results:\")\n",
    "print(f\"   Classified by Gaia: {n_classified} / {len(top_candidates)}\")\n",
    "\n",
    "if n_classified > 0:\n",
    "    print(f\"\\nGaia variability classifications:\")\n",
    "    class_counts = top_candidates['gaia_vari_class'].dropna().value_counts()\n",
    "    for cls, count in class_counts.items():\n",
    "        print(f\"   {cls}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Cross-match with ZTF\n",
    "\n",
    "ZTF provides dense optical light curves with better cadence than Gaia, ideal for resolving short-period orbital modulations and outburst profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Cross-match with ZTF (Zwicky Transient Facility)\n",
    "print(\"=\"*70)\n",
    "print(\"CROSS-MATCHING WITH ZTF (Zwicky Transient Facility)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nChecking {len(top_candidates)} candidates in ZTF catalogs...\")\n",
    "print(\"(ZTF provides dense optical light curves ideal for short-period systems)\")\n",
    "\n",
    "# ZTF-related VizieR catalogs (periodic variables + variable candidates)\n",
    "ztf_catalogs = [\n",
    "    'J/ApJS/249/18',   # ZTF Catalog of Periodic Variable Stars (Chen+ 2020)\n",
    "    'J/AJ/159/198',    # CVs in ZTF Year 1 (Szkody+ 2020)\n",
    "    'J/MNRAS/499/5782', # ZTF DR1 variable source candidates (Ofek+ 2020)\n",
    "]\n",
    "\n",
    "Vizier.ROW_LIMIT = 5\n",
    "first_match_logged = False\n",
    "\n",
    "in_ztf_flags = []\n",
    "ztf_nobs_list = []\n",
    "ztf_period_list = []\n",
    "ztf_catalog_list = []\n",
    "ztf_mag_list = []\n",
    "\n",
    "for i, (idx, row) in enumerate(top_candidates.iterrows()):\n",
    "    coord = SkyCoord(ra=row['ra']*u.deg, dec=row['dec']*u.deg, frame='icrs')\n",
    "    gaia_id = int(row['source_id'])\n",
    "    print(f\"  [{i+1:2d}/{len(top_candidates)}] Gaia DR3 {gaia_id}...\", end=' ', flush=True)\n",
    "\n",
    "    found = False\n",
    "    nobs, period, mag, cat_name = 0, None, None, None\n",
    "\n",
    "    for cat in ztf_catalogs:\n",
    "        try:\n",
    "            result = Vizier(columns=['**'], row_limit=5, timeout=15).query_region(\n",
    "                coord, radius=CONFIG['ztf_search_radius']*u.arcsec, catalog=cat)\n",
    "            if result and len(result) > 0 and len(result[0]) > 0:\n",
    "                table = result[0]\n",
    "                found = True\n",
    "                cat_name = cat\n",
    "\n",
    "                # Log column names on first match for debugging\n",
    "                if not first_match_logged:\n",
    "                    print(f\"\\n    [DEBUG] Matched catalog {cat}, columns: {table.colnames}\")\n",
    "                    first_match_logged = True\n",
    "\n",
    "                r0 = table[0]\n",
    "                # Extract observation counts\n",
    "                # Chen+2020 uses Ng, Nr; others may use nobs, Nobs, etc.\n",
    "                for col in table.colnames:\n",
    "                    cl = col.lower()\n",
    "                    if cl in ('ng', 'nr', 'nobs', 'nepochs', 'numobs', 'ndet', 'n'):\n",
    "                        try:\n",
    "                            val = r0[col]\n",
    "                            if not np.ma.is_masked(val):\n",
    "                                nobs += int(val)\n",
    "                        except: pass\n",
    "                    elif 'nobs' in cl or 'nepoch' in cl or 'ngood' in cl or 'ndet' in cl:\n",
    "                        try:\n",
    "                            val = r0[col]\n",
    "                            if not np.ma.is_masked(val):\n",
    "                                nobs += int(val)\n",
    "                        except: pass\n",
    "\n",
    "                # Extract period - Chen+2020 uses Per-g, Per-r\n",
    "                for col in table.colnames:\n",
    "                    cl = col.lower()\n",
    "                    if cl in ('per', 'period', 'p', 'per-g', 'per-r') or cl.startswith('per'):\n",
    "                        try:\n",
    "                            val = r0[col]\n",
    "                            if not np.ma.is_masked(val) and float(val) > 0:\n",
    "                                period = float(val)\n",
    "                                break\n",
    "                        except: pass\n",
    "\n",
    "                # Extract magnitude\n",
    "                for col in table.colnames:\n",
    "                    cl = col.lower()\n",
    "                    if cl in ('gmag', 'rmag', 'magg', 'magr', 'mag'):\n",
    "                        try:\n",
    "                            val = r0[col]\n",
    "                            if not np.ma.is_masked(val):\n",
    "                                mag = float(val)\n",
    "                                break\n",
    "                        except: pass\n",
    "\n",
    "                parts = []\n",
    "                if nobs > 0: parts.append(f\"{nobs} obs\")\n",
    "                if period: parts.append(f\"P={period:.4f}d\")\n",
    "                if mag: parts.append(f\"mag={mag:.1f}\")\n",
    "                print(f\"ZTF [{cat.split('/')[-1]}]: {', '.join(parts) if parts else 'matched'}\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    if not found:\n",
    "        print(\"not in ZTF catalogs\")\n",
    "\n",
    "    in_ztf_flags.append(found)\n",
    "    ztf_nobs_list.append(nobs)\n",
    "    ztf_period_list.append(period)\n",
    "    ztf_catalog_list.append(cat_name)\n",
    "    ztf_mag_list.append(mag)\n",
    "\n",
    "# Assign directly\n",
    "top_candidates['in_ztf'] = in_ztf_flags\n",
    "top_candidates['ztf_nobs'] = ztf_nobs_list\n",
    "top_candidates['ztf_period'] = ztf_period_list\n",
    "top_candidates['ztf_catalog'] = ztf_catalog_list\n",
    "top_candidates['ztf_mag'] = ztf_mag_list\n",
    "\n",
    "# Drop old split columns if they exist from prior runs\n",
    "for old_col in ['ztf_nobs_g', 'ztf_nobs_r', 'ztf_med_mag_g', 'ztf_med_mag_r']:\n",
    "    if old_col in top_candidates.columns:\n",
    "        top_candidates.drop(columns=[old_col], inplace=True)\n",
    "\n",
    "n_in_ztf = top_candidates['in_ztf'].sum()\n",
    "print(f\"\\n✓ ZTF cross-match complete:\")\n",
    "print(f\"   Found in ZTF catalogs: {n_in_ztf} / {len(top_candidates)}\")\n",
    "\n",
    "if n_in_ztf > 0:\n",
    "    with_period = top_candidates['ztf_period'].notna().sum()\n",
    "    with_nobs = (top_candidates['ztf_nobs'] > 0).sum()\n",
    "    print(f\"   With ZTF periods: {with_period}\")\n",
    "    print(f\"   With observation counts: {with_nobs}\")\n",
    "    for _, r in top_candidates[top_candidates['in_ztf']].iterrows():\n",
    "        parts = []\n",
    "        if r['ztf_nobs'] > 0: parts.append(f\"{r['ztf_nobs']} obs\")\n",
    "        if pd.notna(r['ztf_period']): parts.append(f\"P={r['ztf_period']:.5f}d ({r['ztf_period']*24*60:.1f}min)\")\n",
    "        print(f\"   Gaia DR3 {int(r['source_id'])}: {', '.join(parts) if parts else 'matched'}\")\n",
    "\n",
    "print(f\"\\n   NOTE: VizieR ZTF catalogs only contain periodic variables (Chen+ 2020)\")\n",
    "print(f\"   and known CVs (Szkody+ 2020). Most dwarf novae show aperiodic outbursts\")\n",
    "print(f\"   and won't appear in these catalogs.\")\n",
    "print(f\"   For dense ZTF light curves, request forced photometry from IRSA:\")\n",
    "print(f\"   https://irsa.ipac.caltech.edu/cgi-bin/ZTF/nph_light_curves\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 13: Cross-match with GALEX (UV)\n",
    "\n",
    "UV excess from a hot accretion disk is a strong CV indicator. GALEX FUV/NUV detections are especially diagnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Cross-match with GALEX for UV emission\n",
    "print(\"=\"*70)\n",
    "print(\"CROSS-MATCHING WITH GALEX (UV Survey)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nChecking {len(top_candidates)} candidates for UV counterparts...\")\n",
    "print(\"(UV excess indicates hot accretion disk - strong CV indicator)\")\n",
    "\n",
    "galex_catalog = 'II/335/galex_ais'\n",
    "Vizier.ROW_LIMIT = 3\n",
    "\n",
    "in_galex_flags = []\n",
    "galex_fuv_list = []\n",
    "galex_nuv_list = []\n",
    "galex_fuv_nuv_list = []\n",
    "\n",
    "for i, (idx, row) in enumerate(top_candidates.iterrows()):\n",
    "    coord = SkyCoord(ra=row['ra']*u.deg, dec=row['dec']*u.deg, frame='icrs')\n",
    "    print(f\"  [{i+1:2d}/{len(top_candidates)}] Gaia DR3 {int(row['source_id'])}...\", end=' ', flush=True)\n",
    "    \n",
    "    found = False\n",
    "    fuv, nuv, fuv_nuv = None, None, None\n",
    "    \n",
    "    try:\n",
    "        result = Vizier.query_region(coord, radius=CONFIG['galex_search_radius']*u.arcsec,\n",
    "                                     catalog=galex_catalog)\n",
    "        \n",
    "        if result and len(result) > 0 and len(result[0]) > 0:\n",
    "            galex = result[0][0]\n",
    "            found = True\n",
    "            fuv = float(galex['FUVmag']) if 'FUVmag' in galex.colnames and not np.ma.is_masked(galex['FUVmag']) else None\n",
    "            nuv = float(galex['NUVmag']) if 'NUVmag' in galex.colnames and not np.ma.is_masked(galex['NUVmag']) else None\n",
    "            fuv_nuv = (fuv - nuv) if (fuv is not None and nuv is not None) else None\n",
    "            \n",
    "            parts = []\n",
    "            if fuv is not None: parts.append(f\"FUV={fuv:.1f}\")\n",
    "            if nuv is not None: parts.append(f\"NUV={nuv:.1f}\")\n",
    "            print(' '.join(parts) if parts else \"detected (no mags)\")\n",
    "        else:\n",
    "            print(\"not in GALEX\")\n",
    "    except Exception as e:\n",
    "        print(f\"error: {str(e)[:40]}\")\n",
    "    \n",
    "    in_galex_flags.append(found)\n",
    "    galex_fuv_list.append(fuv)\n",
    "    galex_nuv_list.append(nuv)\n",
    "    galex_fuv_nuv_list.append(fuv_nuv)\n",
    "\n",
    "# Assign directly to avoid merge issues\n",
    "top_candidates['in_galex'] = in_galex_flags\n",
    "top_candidates['galex_fuv'] = galex_fuv_list\n",
    "top_candidates['galex_nuv'] = galex_nuv_list\n",
    "top_candidates['galex_fuv_nuv'] = galex_fuv_nuv_list\n",
    "\n",
    "n_galex = top_candidates['in_galex'].sum()\n",
    "n_fuv = top_candidates['galex_fuv'].notna().sum()\n",
    "n_nuv = top_candidates['galex_nuv'].notna().sum()\n",
    "\n",
    "print(f\"\\n✓ GALEX cross-match complete:\")\n",
    "print(f\"   Any UV detection: {n_galex} / {len(top_candidates)}\")\n",
    "print(f\"   FUV detections: {n_fuv}\")\n",
    "print(f\"   NUV detections: {n_nuv}\")\n",
    "\n",
    "if n_fuv > 0:\n",
    "    print(f\"\\n   ⭐ FUV-detected sources are HIGH PRIORITY CV candidates!\")\n",
    "    fuv_sources = top_candidates[top_candidates['galex_fuv'].notna()]\n",
    "    for _, r in fuv_sources.iterrows():\n",
    "        fuv_str = f\"FUV={r['galex_fuv']:.1f}\" if pd.notna(r['galex_fuv']) else \"\"\n",
    "        nuv_str = f\"NUV={r['galex_nuv']:.1f}\" if pd.notna(r['galex_nuv']) else \"\"\n",
    "        print(f\"      Gaia DR3 {int(r['source_id'])}: {fuv_str} {nuv_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 14: Multi-Wavelength Priority Scoring\n",
    "\n",
    "Scoring incorporates evidence from all cross-matches: anomaly score, X-ray, UV, SIMBAD, Gaia variability class, ZTF coverage, TESS coverage, period, and light curve shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Calculate priority score and create final table\n",
    "print(\"=\"*70)\n",
    "print(\"MULTI-WAVELENGTH PRIORITY SCORING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def calculate_priority(row):\n",
    "    score = row['combined_score'] * 5  # Base anomaly score\n",
    "\n",
    "    # X-ray detection is strong CV indicator (ROSAT)\n",
    "    if row.get('has_xray', False):\n",
    "        score += 3\n",
    "\n",
    "    # GALEX UV detection - hot accretion disk indicator\n",
    "    if pd.notna(row.get('galex_fuv')):\n",
    "        score += 3  # FUV detection is very strong\n",
    "    elif pd.notna(row.get('galex_nuv')):\n",
    "        score += 1.5  # NUV only is still interesting\n",
    "\n",
    "    # SIMBAD classification\n",
    "    if not row.get('in_simbad', True):\n",
    "        score += 5  # Not in SIMBAD - potentially novel discovery (high priority)\n",
    "    else:\n",
    "        simbad_otype = str(row.get('simbad_otype', ''))\n",
    "        if any(t in simbad_otype for t in ['CV', 'No*', 'DN', 'DQ', 'AM', 'NL']):\n",
    "            score += 1  # Confirmed CV-related\n",
    "\n",
    "    # Gaia variability classification\n",
    "    gaia_class = str(row.get('gaia_vari_class', ''))\n",
    "    if gaia_class and gaia_class not in ['None', 'nan', '']:\n",
    "        if any(c in gaia_class for c in ['CV', 'SYST', 'ECL']):\n",
    "            score += 1.5\n",
    "\n",
    "    # Not catalogued or generic in VSX\n",
    "    if not row.get('in_vsx', True):\n",
    "        score += 4  # Not catalogued as variable - potentially novel\n",
    "    elif row.get('vsx_type') in ['VAR', 'VAR:', None, '']:\n",
    "        score += 2.5  # Generic/uncertain type - needs investigation\n",
    "\n",
    "    # ZTF coverage for follow-up potential\n",
    "    ztf_obs = row.get('ztf_nobs', 0) or 0\n",
    "    if ztf_obs > 100:\n",
    "        score += 1.5\n",
    "    elif ztf_obs > 50:\n",
    "        score += 1\n",
    "    elif ztf_obs > 0:\n",
    "        score += 0.5\n",
    "\n",
    "    # ZTF period agreement with VSX\n",
    "    ztf_period = row.get('ztf_period')\n",
    "    vsx_period = row.get('vsx_period')\n",
    "    if pd.notna(ztf_period) and pd.notna(vsx_period) and vsx_period > 0:\n",
    "        if abs(ztf_period - vsx_period) / vsx_period < 0.1:\n",
    "            score += 1  # Period agreement bonus\n",
    "\n",
    "    # TESS coverage for validation\n",
    "    if row.get('tic_id') is not None:\n",
    "        score += 2  # TIC match = TESS observable\n",
    "\n",
    "    # Short period (potential CV)\n",
    "    period = row.get('vsx_period')\n",
    "    if period and period < 0.1:  # < 2.4 hours\n",
    "        score += 2\n",
    "\n",
    "    # Negative skewness (outburst-like)\n",
    "    if row['skewness_mag_g_fov'] < -1:\n",
    "        score += 1\n",
    "\n",
    "    return score\n",
    "\n",
    "top_candidates['priority_score'] = top_candidates.apply(calculate_priority, axis=1)\n",
    "top_candidates = top_candidates.sort_values('priority_score', ascending=False)\n",
    "\n",
    "print(\"\\nPriority scoring criteria:\")\n",
    "print(\"  +5 x anomaly_score (base ML score)\")\n",
    "print(\"  +3 if X-ray detection (ROSAT)\")\n",
    "print(\"  +3 if FUV detection (GALEX), +1.5 if NUV only\")\n",
    "print(\"  +5 if not in SIMBAD (novel discovery), +1 if CV-related SIMBAD type\")\n",
    "print(\"  +1.5 if Gaia vari class matches CV/SYST/ECL\")\n",
    "print(\"  +4 if not in VSX, +2.5 if generic VAR type\")\n",
    "print(\"  +0.5-1.5 based on ZTF observation count\")\n",
    "print(\"  +1 if ZTF period agrees with VSX period\")\n",
    "print(\"  +2 if TIC match (TESS observable)\")\n",
    "print(\"  +2 if period < 2.4 hours\")\n",
    "print(\"  +1 if negative skewness (outburst-like)\")\n",
    "\n",
    "# Filter by priority score threshold\n",
    "priority_threshold = CONFIG['priority_score_min']\n",
    "high_priority = top_candidates[top_candidates['priority_score'] >= priority_threshold]\n",
    "n_hp = len(high_priority)\n",
    "\n",
    "print(f\"\\n  Candidates above priority threshold ({priority_threshold}): {n_hp} / {len(top_candidates)}\")\n",
    "print(f\"  Score range: {top_candidates['priority_score'].min():.1f} - {top_candidates['priority_score'].max():.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"PRIORITIZED CANDIDATES (priority_score >= {priority_threshold}): {n_hp} found\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, (idx, row) in enumerate(high_priority.iterrows()):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RANK #{i+1} | Priority Score: {row['priority_score']:.2f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Gaia DR3 {int(row['source_id'])}\")\n",
    "    print(f\"  Position: RA={row['ra']:.5f}, Dec={row['dec']:.5f}\")\n",
    "    print(f\"  G={row['phot_g_mean_mag']:.2f}, BP-RP={row['bp_rp']:.2f}\")\n",
    "    print(f\"  Amplitude: {row['amplitude']:.2f} mag\")\n",
    "    print(f\"  Skewness: {row['skewness_mag_g_fov']:.2f}, Kurtosis: {row['kurtosis_mag_g_fov']:.2f}\")\n",
    "\n",
    "    # VSX\n",
    "    vsx_str = f\"Yes - {row['vsx_type']}\" if row.get('in_vsx') else 'NOT CATALOGUED'\n",
    "    if row.get('vsx_period'):\n",
    "        vsx_str += f\" (P={row['vsx_period']:.4f}d = {row['vsx_period']*24*60:.1f} min)\"\n",
    "    print(f\"  VSX: {vsx_str}\")\n",
    "\n",
    "    # SIMBAD\n",
    "    if row.get('in_simbad'):\n",
    "        print(f\"  SIMBAD: {row['simbad_otype']} ({row['simbad_name']})\")\n",
    "    else:\n",
    "        print(f\"  SIMBAD: NOT FOUND\")\n",
    "\n",
    "    # Gaia variability\n",
    "    gaia_class = row.get('gaia_vari_class')\n",
    "    if pd.notna(gaia_class) and str(gaia_class) not in ['None', 'nan', '']:\n",
    "        print(f\"  Gaia Vari: {gaia_class} (score={row.get('gaia_vari_score', 0):.2f})\")\n",
    "    else:\n",
    "        print(f\"  Gaia Vari: Not classified\")\n",
    "\n",
    "    # X-ray (ROSAT)\n",
    "    xray_str = 'YES' if row.get('has_xray') else 'No'\n",
    "    if row.get('has_xray') and pd.notna(row.get('xray_count_rate')):\n",
    "        xray_str += f\" (rate={row['xray_count_rate']:.3f} ct/s)\"\n",
    "    print(f\"  X-ray: {xray_str}\")\n",
    "\n",
    "    # GALEX UV\n",
    "    fuv_str = f\"FUV={row['galex_fuv']:.1f}\" if pd.notna(row.get('galex_fuv')) else \"No FUV\"\n",
    "    nuv_str = f\"NUV={row['galex_nuv']:.1f}\" if pd.notna(row.get('galex_nuv')) else \"No NUV\"\n",
    "    print(f\"  GALEX: {fuv_str}, {nuv_str}\")\n",
    "\n",
    "    # ZTF\n",
    "    ztf_nobs = int(row.get('ztf_nobs', 0) or 0)\n",
    "    ztf_per = row.get('ztf_period')\n",
    "    if row.get('in_ztf'):\n",
    "        ztf_parts = [f\"{ztf_nobs} obs\" if ztf_nobs > 0 else \"matched\"]\n",
    "        if pd.notna(ztf_per):\n",
    "            ztf_parts.append(f\"P={ztf_per:.5f}d\")\n",
    "        print(f\"  ZTF: {', '.join(ztf_parts)}\")\n",
    "    else:\n",
    "        print(f\"  ZTF: No\")\n",
    "\n",
    "    # TESS\n",
    "    tic_str = f\"TIC {int(row['tic_id'])}\" if pd.notna(row.get('tic_id')) else 'N/A'\n",
    "    print(f\"  TESS: {tic_str}, {int(row.get('n_tess_sectors', 0))} sectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 15: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Save results to files\n",
    "print(\"=\"*70)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
    "\n",
    "# Save full candidate table\n",
    "output_file = f'cv_candidates_{timestamp}.csv'\n",
    "top_candidates.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved {len(top_candidates)} candidates to {output_file}\")\n",
    "\n",
    "# Save high-priority subset\n",
    "high_priority = top_candidates[top_candidates['priority_score'] >= CONFIG['priority_score_min']]\n",
    "hp_file = f'cv_candidates_high_priority_{timestamp}.csv'\n",
    "high_priority.to_csv(hp_file, index=False)\n",
    "print(f\"✓ Saved {len(high_priority)} high-priority candidates to {hp_file}\")\n",
    "\n",
    "# Summary stats\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"MULTI-WAVELENGTH CROSS-MATCH SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal sources queried: {len(df)}\")\n",
    "print(f\"Sources after cleaning: {len(df_clean)}\")\n",
    "print(f\"Top candidates analyzed: {len(top_candidates)}\")\n",
    "\n",
    "print(f\"\\nCross-match results:\")\n",
    "print(f\"  VSX matches:        {top_candidates['in_vsx'].sum()} / {len(top_candidates)}\")\n",
    "print(f\"  SIMBAD matches:     {top_candidates['in_simbad'].sum()} / {len(top_candidates)}\")\n",
    "gaia_vari_n = top_candidates['gaia_vari_class'].notna().sum() if 'gaia_vari_class' in top_candidates.columns else 0\n",
    "print(f\"  Gaia vari classes:  {gaia_vari_n} / {len(top_candidates)}\")\n",
    "print(f\"  ROSAT X-ray:        {top_candidates['has_xray'].sum()} / {len(top_candidates)}\")\n",
    "galex_n = top_candidates['in_galex'].sum() if 'in_galex' in top_candidates.columns else 0\n",
    "print(f\"  GALEX UV:           {galex_n} / {len(top_candidates)}\")\n",
    "ztf_n = top_candidates['in_ztf'].sum() if 'in_ztf' in top_candidates.columns else 0\n",
    "print(f\"  ZTF coverage:       {ztf_n} / {len(top_candidates)}\")\n",
    "print(f\"  TESS coverage:      {(top_candidates['n_tess_sectors'] > 0).sum()} / {len(top_candidates)}\")\n",
    "\n",
    "# Top 5 with evidence summary\n",
    "best = top_candidates[top_candidates['priority_score'] >= CONFIG['priority_score_min']].head(10)\n",
    "print(f\"\\nTOP 5 CANDIDATES TO INVESTIGATE:\")\n",
    "for i, (idx, row) in enumerate(best.iterrows()):\n",
    "    flags = []\n",
    "    if row.get('has_xray'): flags.append('X-ray')\n",
    "    if pd.notna(row.get('galex_fuv')): flags.append('FUV')\n",
    "    elif pd.notna(row.get('galex_nuv')): flags.append('NUV')\n",
    "    if not row.get('in_vsx'): flags.append('Not-in-VSX')\n",
    "    elif row.get('vsx_type') == 'VAR': flags.append('Generic-VAR')\n",
    "    if not row.get('in_simbad'): flags.append('Not-in-SIMBAD')\n",
    "    if row['skewness_mag_g_fov'] < -1: flags.append('Outburst-like')\n",
    "    gaia_class = str(row.get('gaia_vari_class', ''))\n",
    "    if gaia_class and gaia_class not in ['None', 'nan', '']:\n",
    "        flags.append(f'Gaia:{gaia_class}')\n",
    "    \n",
    "    print(f\"  {i+1}. Gaia DR3 {int(row['source_id'])} (TIC {int(row['tic_id']) if pd.notna(row.get('tic_id')) else 'N/A'})\")\n",
    "    print(f\"     G={row['phot_g_mean_mag']:.1f}, Amp={row['amplitude']:.2f}mag, Score={row['priority_score']:.1f}\")\n",
    "    print(f\"     Evidence: {', '.join(flags) if flags else 'ML anomaly only'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 16: Extract TESS Light Curves for High-Priority Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Extract TESS light curves for top 10 candidates\n",
    "import time as _time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TESS LIGHT CURVE EXTRACTION: TOP 10 CANDIDATES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select candidates by priority score threshold + TESS coverage\n",
    "priority_threshold = CONFIG['priority_score_min']\n",
    "candidates_with_tess = top_candidates[\n",
    "    (top_candidates['n_tess_sectors'] > 0) &\n",
    "    (top_candidates['priority_score'] >= priority_threshold)\n",
    "].copy()\n",
    "n_extract = len(candidates_with_tess)\n",
    "\n",
    "print(f\"\\nExtracting light curves for {n_extract} candidates\")\n",
    "print(f\"  (priority_score >= {priority_threshold} AND TESS coverage > 0)\")\n",
    "\n",
    "n_rows = (n_extract + 1) // 2\n",
    "fig, axes = plt.subplots(n_rows, 2, figsize=(18, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "lc_data = {}  # Store light curve data for later analysis\n",
    "\n",
    "def fetch_tess_cutout(coord, size, sector, max_retries=3):\n",
    "    \"\"\"Fetch TESS cutout with retry logic for transient network errors.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            cutout = Tesscut.get_cutouts(coordinates=coord, size=size, sector=sector)\n",
    "            return cutout\n",
    "        except Exception as e:\n",
    "            err_str = str(e).lower()\n",
    "            if any(kw in err_str for kw in ['disconnect', 'connection', 'timeout', 'reset', 'refused']):\n",
    "                if attempt < max_retries - 1:\n",
    "                    wait = 5 * (attempt + 1)\n",
    "                    print(f\"  Network error, retrying in {wait}s (attempt {attempt+2}/{max_retries})...\", flush=True)\n",
    "                    _time.sleep(wait)\n",
    "                    continue\n",
    "            raise  # Non-network errors or final attempt: re-raise\n",
    "\n",
    "for i, (idx, target) in enumerate(candidates_with_tess.iterrows()):\n",
    "    ax = axes[i]\n",
    "    tic_id = int(target['tic_id']) if pd.notna(target.get('tic_id')) else 0\n",
    "    gaia_id = int(target['source_id'])\n",
    "\n",
    "    print(f\"\\n[{i+1}/{n_extract}] TIC {tic_id} (Gaia DR3 {gaia_id})\")\n",
    "\n",
    "    coord = SkyCoord(ra=target['ra']*u.deg, dec=target['dec']*u.deg, frame='icrs')\n",
    "    sectors = target['tess_sectors']\n",
    "\n",
    "    if not sectors or len(sectors) == 0:\n",
    "        ax.text(0.5, 0.5, 'No sector data', ha='center', va='center', transform=ax.transAxes)\n",
    "        ax.set_title(f'#{i+1} TIC {tic_id}', fontsize=10)\n",
    "        continue\n",
    "\n",
    "    sector = sectors[0]\n",
    "\n",
    "    try:\n",
    "        cutout = fetch_tess_cutout(coord, size=5, sector=sector)\n",
    "\n",
    "        if cutout and len(cutout) > 0:\n",
    "            hdu = cutout[0]\n",
    "            data = hdu[1].data\n",
    "\n",
    "            time = data['TIME']\n",
    "            flux_cube = data['FLUX']\n",
    "\n",
    "            # Simple aperture photometry (3x3 central pixels)\n",
    "            aperture_flux = np.sum(flux_cube[:, 1:4, 1:4], axis=(1, 2))\n",
    "\n",
    "            # Clean bad data\n",
    "            good = np.isfinite(aperture_flux) & (aperture_flux > 0)\n",
    "            time_clean = time[good]\n",
    "            flux_clean = aperture_flux[good]\n",
    "\n",
    "            if len(flux_clean) == 0:\n",
    "                ax.text(0.5, 0.5, 'No valid data', ha='center', va='center', transform=ax.transAxes)\n",
    "                ax.set_title(f'#{i+1} TIC {tic_id}', fontsize=10)\n",
    "                continue\n",
    "\n",
    "            # Normalize\n",
    "            flux_norm = flux_clean / np.nanmedian(flux_clean)\n",
    "\n",
    "            # Store for later analysis\n",
    "            lc_data[tic_id] = {\n",
    "                'time': time_clean,\n",
    "                'flux': flux_norm,\n",
    "                'sector': sector,\n",
    "                'gaia_id': gaia_id\n",
    "            }\n",
    "\n",
    "            # Plot\n",
    "            ax.scatter(time_clean, flux_norm, s=1, alpha=0.5, c='darkblue')\n",
    "\n",
    "            # Mark outliers (potential outbursts)\n",
    "            std = np.nanstd(flux_norm)\n",
    "            med = np.nanmedian(flux_norm)\n",
    "            outliers = (flux_norm > med + 3*std) | (flux_norm < med - 3*std)\n",
    "            if np.any(outliers):\n",
    "                ax.scatter(time_clean[outliers], flux_norm[outliers], s=8, c='red', zorder=10)\n",
    "\n",
    "            # Title with VSX type and key flags\n",
    "            vsx_info = f\" [{target['vsx_type']}]\" if target.get('in_vsx') and target.get('vsx_type') else \"\"\n",
    "            flags = []\n",
    "            if target.get('has_xray'): flags.append('Xray')\n",
    "            if pd.notna(target.get('galex_fuv')): flags.append('FUV')\n",
    "            elif pd.notna(target.get('galex_nuv')): flags.append('NUV')\n",
    "            flag_str = f\" ({','.join(flags)})\" if flags else \"\"\n",
    "\n",
    "            ax.set_title(f'#{i+1} TIC {tic_id}{vsx_info}{flag_str} | S{sector}', fontsize=10)\n",
    "            ax.set_ylabel('Norm. Flux', fontsize=8)\n",
    "            ax.tick_params(labelsize=8)\n",
    "            ax.grid(True, alpha=0.2)\n",
    "\n",
    "            # Annotate with key stats\n",
    "            n_outliers = np.sum(outliers)\n",
    "            max_excursion = (flux_norm.max() - med) / std if std > 0 else 0\n",
    "            ax.text(0.02, 0.95, f'N={len(flux_norm)}, Out={n_outliers}, Max={max_excursion:.1f}\\u03c3',\n",
    "                   transform=ax.transAxes, fontsize=7, va='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "            print(f\"  Sector {sector}: {len(flux_norm)} pts, {n_outliers} outliers, max {max_excursion:.1f}\\u03c3\")\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No cutout data', ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title(f'#{i+1} TIC {tic_id}', fontsize=10)\n",
    "\n",
    "    except Exception as e:\n",
    "        ax.text(0.5, 0.5, f'Error: {str(e)[:40]}', ha='center', va='center',\n",
    "                transform=ax.transAxes, fontsize=7)\n",
    "        ax.set_title(f'#{i+1} TIC {tic_id}', fontsize=10)\n",
    "        print(f\"  Error: {e}\")\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "# Common x-label for bottom row\n",
    "for ax in axes[-2:]:\n",
    "    if ax.get_visible():\n",
    "        ax.set_xlabel('TESS BJD (days)', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "lc_file = 'tess_lc_top10.png'\n",
    "plt.savefig(lc_file, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Light curves saved to {lc_file}\")\n",
    "print(f\"  Successfully extracted: {len(lc_data)} / {n_extract}\")\n",
    "\n",
    "# Summary of interesting features\n",
    "print(f\"\\nLight curve assessment:\")\n",
    "for tic_id, lc in lc_data.items():\n",
    "    flux = lc['flux']\n",
    "    std = np.nanstd(flux)\n",
    "    med = np.nanmedian(flux)\n",
    "    outliers = np.sum((flux > med + 3*std) | (flux < med - 3*std))\n",
    "    max_exc = (flux.max() - med) / std if std > 0 else 0\n",
    "\n",
    "    flags = []\n",
    "    if outliers > 10 and max_exc > 5: flags.append('POTENTIAL OUTBURST')\n",
    "    if std > 0.05: flags.append('HIGH VARIABILITY')\n",
    "    if max_exc > 10: flags.append('EXTREME EXCURSION')\n",
    "\n",
    "    if flags:\n",
    "        print(f\"  TIC {tic_id}: {', '.join(flags)} (max={max_exc:.1f}\\u03c3, outliers={outliers})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 17: Deep Candidate Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Deep Candidate Investigation - Extended Parameters & Classification\n",
    "print(\"=\"*70)\n",
    "print(\"DEEP CANDIDATE INVESTIGATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Investigate all candidates above priority threshold\n",
    "priority_threshold = CONFIG['priority_score_min']\n",
    "deep_candidates = top_candidates[top_candidates['priority_score'] >= priority_threshold].copy()\n",
    "n_deep = len(deep_candidates)\n",
    "\n",
    "print(f\"\\nQuerying extended Gaia DR3 parameters for {n_deep} candidates\")\n",
    "print(f\"  (priority_score >= {priority_threshold})\")\n",
    "\n",
    "# --- Extended Gaia DR3 parameters ---\n",
    "gaia_ids = [str(int(sid)) for sid in deep_candidates['source_id']]\n",
    "id_list = ', '.join(gaia_ids)\n",
    "\n",
    "extended_query = f\"\"\"\n",
    "SELECT source_id, ra, dec, parallax, parallax_error, pmra, pmdec,\n",
    "       phot_g_mean_mag, bp_rp, teff_gspphot, logg_gspphot,\n",
    "       mh_gspphot, radial_velocity, ruwe,\n",
    "       astrometric_excess_noise, phot_bp_rp_excess_factor,\n",
    "       phot_variable_flag, non_single_star\n",
    "FROM gaiadr3.gaia_source\n",
    "WHERE source_id IN ({id_list})\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    from astroquery.gaia import Gaia\n",
    "    job = Gaia.launch_job(extended_query)\n",
    "    ext_results = job.get_results()\n",
    "    print(f\"  Retrieved extended parameters for {len(ext_results)} sources\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error querying extended params: {e}\")\n",
    "    ext_results = None\n",
    "\n",
    "# --- CV-specific catalog searches ---\n",
    "print(f\"\\nSearching CV-specific catalogs...\")\n",
    "\n",
    "cv_catalog_results = {}\n",
    "for i, (idx, row) in enumerate(deep_candidates.iterrows()):\n",
    "    gaia_id = int(row['source_id'])\n",
    "    coord = SkyCoord(ra=row['ra']*u.deg, dec=row['dec']*u.deg, frame='icrs')\n",
    "    print(f\"\\n  [{i+1}/{n_deep}] Gaia DR3 {gaia_id}\")\n",
    "    \n",
    "    info = {'gaia_id': gaia_id}\n",
    "    \n",
    "    # Extended Gaia params\n",
    "    if ext_results is not None:\n",
    "        match = ext_results[ext_results['source_id'] == gaia_id]\n",
    "        if len(match) > 0:\n",
    "            m = match[0]\n",
    "            teff = m['teff_gspphot'] if not np.ma.is_masked(m['teff_gspphot']) else None\n",
    "            logg = m['logg_gspphot'] if not np.ma.is_masked(m['logg_gspphot']) else None\n",
    "            rv = m['radial_velocity'] if not np.ma.is_masked(m['radial_velocity']) else None\n",
    "            ruwe = m['ruwe'] if not np.ma.is_masked(m['ruwe']) else None\n",
    "            plx = m['parallax'] if not np.ma.is_masked(m['parallax']) else None\n",
    "            nsstar = m['non_single_star'] if not np.ma.is_masked(m['non_single_star']) else None\n",
    "            \n",
    "            info.update({'teff': teff, 'logg': logg, 'rv': rv, 'ruwe': ruwe,\n",
    "                        'parallax': plx, 'non_single_star': nsstar})\n",
    "            \n",
    "            if teff: print(f\"    Teff = {teff:.0f} K\", end='')\n",
    "            if logg: print(f\", log(g) = {logg:.2f}\", end='')\n",
    "            if rv: print(f\", RV = {rv:.1f} km/s\", end='')\n",
    "            if ruwe: print(f\", RUWE = {ruwe:.2f}\", end='')\n",
    "            print()\n",
    "            \n",
    "            if plx and plx > 0:\n",
    "                dist_pc = 1000.0 / plx\n",
    "                abs_g = m['phot_g_mean_mag'] + 5 * np.log10(plx / 100)\n",
    "                info['dist_pc'] = dist_pc\n",
    "                info['abs_g'] = abs_g\n",
    "                print(f\"    Distance ~ {dist_pc:.0f} pc, M_G = {abs_g:.2f}\")\n",
    "    \n",
    "    # Search XMM-Newton (4XMM-DR14)\n",
    "    try:\n",
    "        xmm = Vizier(columns=['*'], row_limit=3, timeout=10).query_region(\n",
    "            coord, radius=10*u.arcsec, catalog='IX/68/xmm4d14s')\n",
    "        if xmm and len(xmm) > 0 and len(xmm[0]) > 0:\n",
    "            info['xmm_detected'] = True\n",
    "            info['xmm_matches'] = len(xmm[0])\n",
    "            print(f\"    XMM-Newton: {len(xmm[0])} detection(s)\")\n",
    "        else:\n",
    "            info['xmm_detected'] = False\n",
    "    except:\n",
    "        info['xmm_detected'] = None\n",
    "    \n",
    "    # Search eROSITA (if available via VizieR)\n",
    "    try:\n",
    "        erosita = Vizier(columns=['*'], row_limit=3, timeout=10).query_region(\n",
    "            coord, radius=15*u.arcsec, catalog='J/A+A/661/A1')\n",
    "        if erosita and len(erosita) > 0 and len(erosita[0]) > 0:\n",
    "            info['erosita_detected'] = True\n",
    "            print(f\"    eROSITA: detected\")\n",
    "        else:\n",
    "            info['erosita_detected'] = False\n",
    "    except:\n",
    "        info['erosita_detected'] = None\n",
    "    \n",
    "    # Search SDSS spectroscopy\n",
    "    try:\n",
    "        sdss_spec = Vizier(columns=['*'], row_limit=3, timeout=10).query_region(\n",
    "            coord, radius=3*u.arcsec, catalog='V/154/sdss16')\n",
    "        if sdss_spec and len(sdss_spec) > 0 and len(sdss_spec[0]) > 0:\n",
    "            info['sdss_spec'] = True\n",
    "            print(f\"    SDSS spectroscopy: available\")\n",
    "        else:\n",
    "            info['sdss_spec'] = False\n",
    "    except:\n",
    "        info['sdss_spec'] = None\n",
    "    \n",
    "    # --- Physical interpretation ---\n",
    "    classification = []\n",
    "    confidence_flags = []\n",
    "    \n",
    "    # Check color-magnitude position\n",
    "    bp_rp = row.get('bp_rp', None)\n",
    "    if bp_rp is not None and bp_rp < 0.5:\n",
    "        classification.append('Blue color consistent with hot accretion disk or WD')\n",
    "        confidence_flags.append('blue')\n",
    "    \n",
    "    # Check X-ray + UV combination\n",
    "    has_xray = row.get('has_xray', False) or info.get('xmm_detected', False)\n",
    "    has_fuv = pd.notna(row.get('galex_fuv'))\n",
    "    if has_xray and has_fuv:\n",
    "        classification.append('X-ray + UV: Strong accretion indicator')\n",
    "        confidence_flags.append('xray+uv')\n",
    "    elif has_xray:\n",
    "        classification.append('X-ray detected: possible magnetic CV or coronal emitter')\n",
    "        confidence_flags.append('xray')\n",
    "    elif has_fuv:\n",
    "        classification.append('FUV detected: hot component present')\n",
    "        confidence_flags.append('fuv')\n",
    "    \n",
    "    # Check RUWE for binarity\n",
    "    if info.get('ruwe') and info['ruwe'] > 1.4:\n",
    "        classification.append(f'High RUWE ({info[\"ruwe\"]:.2f}): astrometric binary signature')\n",
    "        confidence_flags.append('binary')\n",
    "    \n",
    "    # Check Teff\n",
    "    if info.get('teff'):\n",
    "        if info['teff'] > 10000:\n",
    "            classification.append(f'Hot (Teff={info[\"teff\"]:.0f}K): WD/sdO/sdB or accretion-heated')\n",
    "        elif info['teff'] < 4500:\n",
    "            classification.append(f'Cool (Teff={info[\"teff\"]:.0f}K): likely M-dwarf donor or flare star')\n",
    "    \n",
    "    # Check variability amplitude\n",
    "    if row.get('phot_g_n_obs', 0) > 0 and row.get('std_dev_over_rms_err_mag', 0) > 5:\n",
    "        classification.append('High photometric variability (>5x expected)')\n",
    "        confidence_flags.append('var')\n",
    "    \n",
    "    # Overall assessment\n",
    "    n_cv_indicators = len(confidence_flags)\n",
    "    if n_cv_indicators >= 3:\n",
    "        assessment = 'STRONG CV CANDIDATE'\n",
    "    elif n_cv_indicators >= 2:\n",
    "        assessment = 'MODERATE CV CANDIDATE'\n",
    "    elif n_cv_indicators >= 1:\n",
    "        assessment = 'WEAK CV CANDIDATE'\n",
    "    else:\n",
    "        assessment = 'UNCERTAIN - needs more data'\n",
    "    \n",
    "    info['classification_notes'] = classification\n",
    "    info['assessment'] = assessment\n",
    "    info['n_cv_indicators'] = n_cv_indicators\n",
    "    cv_catalog_results[gaia_id] = info\n",
    "    \n",
    "    print(f\"    Assessment: {assessment}\")\n",
    "    for note in classification:\n",
    "        print(f\"      - {note}\")\n",
    "    \n",
    "    # Generate finder chart / archive links\n",
    "    ra_str = f\"{row['ra']:.6f}\"\n",
    "    dec_str = f\"{row['dec']:.6f}\"\n",
    "    print(f\"    Links:\")\n",
    "    print(f\"      Aladin: https://aladin.cds.unistra.fr/AladinLite/?target={ra_str}+{dec_str}&fov=0.05\")\n",
    "    print(f\"      ESASky: https://sky.esa.int/?target={ra_str}%20{dec_str}&hips=DSS2+color&fov=0.05\")\n",
    "\n",
    "# --- Summary table ---\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DEEP INVESTIGATION SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"{'Gaia DR3 ID':<22} {'Teff':>6} {'log(g)':>7} {'RUWE':>6} {'XMM':>5} {'eROS':>5} {'SDSS':>5} {'Assessment'}\")\n",
    "print(\"-\" * 90)\n",
    "for gaia_id, info in cv_catalog_results.items():\n",
    "    teff_s = f\"{info.get('teff', 0):.0f}\" if info.get('teff') else '---'\n",
    "    logg_s = f\"{info.get('logg', 0):.2f}\" if info.get('logg') else '---'\n",
    "    ruwe_s = f\"{info.get('ruwe', 0):.2f}\" if info.get('ruwe') else '---'\n",
    "    xmm_s = 'Y' if info.get('xmm_detected') else ('?' if info.get('xmm_detected') is None else 'N')\n",
    "    eros_s = 'Y' if info.get('erosita_detected') else ('?' if info.get('erosita_detected') is None else 'N')\n",
    "    sdss_s = 'Y' if info.get('sdss_spec') else ('?' if info.get('sdss_spec') is None else 'N')\n",
    "    print(f\"{gaia_id:<22} {teff_s:>6} {logg_s:>7} {ruwe_s:>6} {xmm_s:>5} {eros_s:>5} {sdss_s:>5} {info['assessment']}\")\n",
    "\n",
    "# Store for use in priority refinement\n",
    "top_candidates_deep = cv_catalog_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 18: Period Analysis (Lomb-Scargle + Phase Folding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Period Analysis for Top Candidates with TESS Data\n",
    "from astropy.timeseries import LombScargle\n",
    "from scipy.ndimage import median_filter\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PERIOD ANALYSIS: LOMB-SCARGLE + PHASE FOLDING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "n_period = len(lc_data)\n",
    "print(f\"\\nAnalyzing all {n_period} candidates with extracted TESS light curves...\")\n",
    "print(\"Strategy: detrend outbursts first, then search for orbital period\")\n",
    "\n",
    "period_results = {}\n",
    "\n",
    "fig, axes = plt.subplots(max(n_period, 1), 2, figsize=(16, 4.5 * max(n_period, 1)), squeeze=False)\n",
    "\n",
    "for plot_idx, (tic_id, lc) in enumerate(list(lc_data.items())[:n_period]):\n",
    "    ax_lc = axes[plot_idx][0]   # left: detrended light curve\n",
    "    ax_ls = axes[plot_idx][1]   # right: periodogram\n",
    "    t = lc['time']\n",
    "    flux = lc['flux']\n",
    "    gaia_id = lc['gaia_id']\n",
    "\n",
    "    print(f\"\\n[{plot_idx+1}/{n_period}] TIC {tic_id} (Gaia DR3 {gaia_id})\")\n",
    "\n",
    "    # Look up VSX period\n",
    "    cand_row = top_candidates[top_candidates['source_id'] == gaia_id]\n",
    "    vsx_period = None\n",
    "    if len(cand_row) > 0 and pd.notna(cand_row.iloc[0].get('vsx_period')):\n",
    "        vsx_period = cand_row.iloc[0]['vsx_period']\n",
    "\n",
    "    # Calculate cadence and Nyquist limit\n",
    "    dt = np.diff(np.sort(t))\n",
    "    dt_clean = dt[dt > 0]\n",
    "    median_cadence = np.median(dt_clean) if len(dt_clean) > 0 else 30.0 / (24 * 60)\n",
    "    nyquist_period = 2 * median_cadence\n",
    "    print(f\"  Cadence: {median_cadence*24*60:.1f} min -> Nyquist: {nyquist_period*24*60:.1f} min\")\n",
    "\n",
    "    # --- Step 1: Identify and remove outburst data ---\n",
    "    med = np.nanmedian(flux)\n",
    "    std = np.nanstd(flux)\n",
    "\n",
    "    # For dwarf novae: outbursts are ABOVE quiescence (brightening = flux increase)\n",
    "    # Use iterative sigma clipping to find quiescent level\n",
    "    quiescent_mask = np.ones(len(flux), dtype=bool)\n",
    "    for clip_iter in range(3):\n",
    "        q_med = np.nanmedian(flux[quiescent_mask])\n",
    "        q_std = np.nanstd(flux[quiescent_mask])\n",
    "        if q_std <= 0:\n",
    "            break\n",
    "        # Clip points >2.5 sigma above quiescent median (outbursts)\n",
    "        # Keep points below (dips/eclipses are useful for period finding)\n",
    "        quiescent_mask = flux < (q_med + 2.5 * q_std)\n",
    "\n",
    "    t_q = t[quiescent_mask]\n",
    "    flux_q = flux[quiescent_mask]\n",
    "    n_outburst = np.sum(~quiescent_mask)\n",
    "    pct_quiescent = 100 * len(t_q) / len(t)\n",
    "    print(f\"  Quiescent points: {len(t_q)}/{len(t)} ({pct_quiescent:.0f}%), outburst points removed: {n_outburst}\")\n",
    "\n",
    "    # --- Step 2: Detrend quiescent data (remove slow variations) ---\n",
    "    if len(t_q) > 50:\n",
    "        # Median filter to remove trends longer than ~1 day\n",
    "        filter_width = max(3, int(1.0 / median_cadence))  # ~1 day window\n",
    "        if filter_width % 2 == 0:\n",
    "            filter_width += 1\n",
    "        flux_trend = median_filter(flux_q, size=min(filter_width, len(flux_q) // 3 * 2 + 1))\n",
    "        flux_detrend = flux_q / flux_trend\n",
    "        # Re-normalize\n",
    "        flux_detrend = flux_detrend / np.nanmedian(flux_detrend)\n",
    "    else:\n",
    "        flux_detrend = flux_q / np.nanmedian(flux_q) if len(flux_q) > 0 else flux_q\n",
    "\n",
    "    # Plot detrended quiescent light curve\n",
    "    ax_lc.scatter(t_q, flux_detrend, s=1, alpha=0.4, c='darkblue', label='Quiescent (detrended)')\n",
    "    outburst_t = t[~quiescent_mask]\n",
    "    outburst_f = flux[~quiescent_mask] / med  # rough normalization for display\n",
    "    ax_lc.scatter(outburst_t, np.full_like(outburst_f, np.nanmax(flux_detrend)*1.02),\n",
    "                  s=8, c='red', marker='v', alpha=0.5, label=f'Outburst ({n_outburst} pts)')\n",
    "    ax_lc.set_xlabel('TESS BJD')\n",
    "    ax_lc.set_ylabel('Detrended Flux')\n",
    "    ax_lc.legend(fontsize=7, loc='upper right')\n",
    "    ax_lc.set_title(f'TIC {tic_id} - Quiescent', fontsize=10)\n",
    "    ax_lc.grid(True, alpha=0.2)\n",
    "\n",
    "    # --- Step 3: Lomb-Scargle on detrended quiescent data ---\n",
    "    if len(t_q) < 30:\n",
    "        ax_ls.text(0.5, 0.5, f'Too few quiescent pts ({len(t_q)})',\n",
    "                   ha='center', va='center', transform=ax_ls.transAxes)\n",
    "        ax_ls.set_title(f'TIC {tic_id} | Insufficient data', fontsize=10)\n",
    "        print(f\"  Skipping L-S: only {len(t_q)} quiescent points\")\n",
    "        continue\n",
    "\n",
    "    min_period = nyquist_period\n",
    "    max_period = min(1.0, (t_q[-1] - t_q[0]) / 3)  # up to 1 day (24 hr) - CV orbital range\n",
    "\n",
    "    if min_period >= max_period:\n",
    "        ax_ls.text(0.5, 0.5, 'Period range too narrow', ha='center', va='center', transform=ax_ls.transAxes)\n",
    "        ax_ls.set_title(f'TIC {tic_id}', fontsize=10)\n",
    "        print(f\"  Skipping: Nyquist ({nyquist_period*24*60:.0f} min) >= max search ({max_period*24*60:.0f} min)\")\n",
    "        continue\n",
    "\n",
    "    # Frequency grid\n",
    "    n_freq = int(10 * (1/min_period - 1/max_period) * (t_q[-1] - t_q[0]))\n",
    "    n_freq = min(max(n_freq, 5000), 100000)\n",
    "    frequency = np.linspace(1/max_period, 1/min_period, n_freq)\n",
    "\n",
    "    try:\n",
    "        ls = LombScargle(t_q, flux_detrend)\n",
    "        power = ls.power(frequency)\n",
    "\n",
    "        # Find top 5 peaks\n",
    "        peak_indices = []\n",
    "        power_copy = power.copy()\n",
    "        for _ in range(5):\n",
    "            if np.max(power_copy) <= 0:\n",
    "                break\n",
    "            peak_idx = np.argmax(power_copy)\n",
    "            peak_indices.append(peak_idx)\n",
    "            peak_freq = frequency[peak_idx]\n",
    "            mask = np.abs(frequency - peak_freq) < 0.03 * peak_freq\n",
    "            power_copy[mask] = 0\n",
    "\n",
    "        best_freq = frequency[peak_indices[0]]\n",
    "        best_period = 1 / best_freq\n",
    "        fap = ls.false_alarm_probability(power.max())\n",
    "\n",
    "        # Check if VSX period matches any of the top peaks (or harmonics)\n",
    "        vsx_match_peak = None\n",
    "        vsx_match_type = None\n",
    "        if vsx_period and vsx_period * 24 * 60 >= nyquist_period * 24 * 60:\n",
    "            for pi in peak_indices:\n",
    "                p = 1/frequency[pi]\n",
    "                for harmonic, label in [(1, '1:1'), (2, '2:1'), (0.5, '1:2'), (3, '3:1'), (1/3, '1:3')]:\n",
    "                    test_p = vsx_period * harmonic\n",
    "                    if test_p > 0 and abs(p - test_p) / test_p < 0.05:\n",
    "                        vsx_match_peak = p\n",
    "                        vsx_match_type = label\n",
    "                        break\n",
    "                if vsx_match_peak:\n",
    "                    break\n",
    "\n",
    "        period_results[tic_id] = {\n",
    "            'best_period_days': best_period,\n",
    "            'best_period_min': best_period * 24 * 60,\n",
    "            'fap': fap,\n",
    "            'vsx_period': vsx_period,\n",
    "            'gaia_id': gaia_id,\n",
    "            'nyquist_min': nyquist_period * 24 * 60,\n",
    "            'vsx_match': vsx_match_peak is not None,\n",
    "            'vsx_match_type': vsx_match_type,\n",
    "            'n_quiescent': len(t_q),\n",
    "            'n_outburst': n_outburst,\n",
    "            'top_periods_min': [1/frequency[pi] * 24 * 60 for pi in peak_indices[:5]]\n",
    "        }\n",
    "\n",
    "        print(f\"  Best L-S period: {best_period*24*60:.2f} min ({best_period:.6f} d)\")\n",
    "        print(f\"  FAP: {fap:.2e}\")\n",
    "        top5_str = ', '.join(f'{1/frequency[pi]*24*60:.1f}' for pi in peak_indices[:5])\n",
    "        print(f\"  Top 5 peaks (min): {top5_str}\")\n",
    "        if vsx_period:\n",
    "            print(f\"  VSX period: {vsx_period*24*60:.1f} min\")\n",
    "            if vsx_match_peak:\n",
    "                print(f\"  VSX MATCH: {vsx_match_type} harmonic (L-S peak at {vsx_match_peak*24*60:.1f} min)\")\n",
    "            else:\n",
    "                if vsx_period * 24 * 60 < nyquist_period * 24 * 60:\n",
    "                    print(f\"  VSX period ({vsx_period*24*60:.0f} min) is BELOW Nyquist limit ({nyquist_period*24*60:.0f} min) - undetectable at this cadence\")\n",
    "                else:\n",
    "                    print(f\"  No match in top L-S peaks\")\n",
    "\n",
    "        # Plot periodogram\n",
    "        period_arr = 1 / frequency\n",
    "        ax_ls.semilogx(period_arr * 24 * 60, power, 'b-', lw=0.5)\n",
    "        ax_ls.axvline(best_period * 24 * 60, color='red', ls='--', lw=1.5,\n",
    "                      label=f'Best: {best_period*24*60:.1f} min')\n",
    "        if vsx_period:\n",
    "            ax_ls.axvline(vsx_period * 24 * 60, color='orange', ls=':', lw=2,\n",
    "                          label=f'VSX: {vsx_period*24*60:.1f} min')\n",
    "            # Also mark 2x harmonic of VSX period\n",
    "            ax_ls.axvline(vsx_period * 2 * 24 * 60, color='orange', ls=':', lw=1, alpha=0.4)\n",
    "        ax_ls.axvline(nyquist_period * 24 * 60, color='gray', ls='-.', lw=1, alpha=0.5,\n",
    "                      label=f'Nyquist: {nyquist_period*24*60:.0f} min')\n",
    "        ax_ls.set_xlabel('Period (min)')\n",
    "        ax_ls.set_ylabel('L-S Power')\n",
    "        fap_str = f'{fap:.1e}' if fap > 0 else '<1e-16'\n",
    "        match_str = f' VSX:{vsx_match_type}' if vsx_match_peak else ''\n",
    "        ax_ls.set_title(f'TIC {tic_id} | FAP={fap_str}{match_str}', fontsize=10)\n",
    "        ax_ls.legend(fontsize=7)\n",
    "        ax_ls.set_xlim(nyquist_period * 24 * 60 * 0.9, max_period * 24 * 60 * 1.1)\n",
    "\n",
    "    except Exception as e:\n",
    "        ax_ls.text(0.5, 0.5, f'Error: {str(e)[:30]}', ha='center', va='center',\n",
    "                   transform=ax_ls.transAxes, fontsize=8)\n",
    "        ax_ls.set_title(f'TIC {tic_id}', fontsize=10)\n",
    "        print(f\"  Error: {e}\")\n",
    "\n",
    "plt.suptitle('Period Analysis: Outburst-Detrended Quiescent Data', fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('periodograms_top5.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"\\n✓ Saved periodograms to periodograms_top5.png\")\n",
    "\n",
    "# Phase-folded light curves for significant periods\n",
    "sig_periods = {k: v for k, v in period_results.items() if v['fap'] < 0.01}\n",
    "if sig_periods:\n",
    "    n_sig = len(sig_periods)\n",
    "    n_phase_cols = min(n_sig, 4)\n",
    "    n_phase_rows = (n_sig + n_phase_cols - 1) // n_phase_cols\n",
    "    fig2, axes2 = plt.subplots(n_phase_rows, n_phase_cols,\n",
    "                                figsize=(5 * n_phase_cols, 4 * n_phase_rows), squeeze=False)\n",
    "\n",
    "    for j, (tic_id, pr) in enumerate(list(sig_periods.items())[:8]):\n",
    "        ax = axes2[j // n_phase_cols][j % n_phase_cols]\n",
    "        t_q_local = t_q if tic_id == list(lc_data.keys())[plot_idx] else None\n",
    "\n",
    "        # Re-extract quiescent data for this target\n",
    "        lc_local = lc_data[tic_id]\n",
    "        t_loc = lc_local['time']\n",
    "        f_loc = lc_local['flux']\n",
    "        q_mask = np.ones(len(f_loc), dtype=bool)\n",
    "        for _ in range(3):\n",
    "            qm = np.nanmedian(f_loc[q_mask])\n",
    "            qs = np.nanstd(f_loc[q_mask])\n",
    "            if qs <= 0: break\n",
    "            q_mask = f_loc < (qm + 2.5 * qs)\n",
    "        t_phase = t_loc[q_mask]\n",
    "        f_phase = f_loc[q_mask] / np.nanmedian(f_loc[q_mask])\n",
    "\n",
    "        # Choose fold period: prefer VSX if matched, else best L-S\n",
    "        fold_period = pr['best_period_days']\n",
    "        period_label = f\"L-S: {pr['best_period_min']:.1f} min\"\n",
    "        if pr['vsx_period'] and pr['vsx_match']:\n",
    "            fold_period = pr['vsx_period']\n",
    "            period_label = f\"VSX: {pr['vsx_period']*24*60:.1f} min\"\n",
    "        elif pr['vsx_period'] and pr['vsx_period'] * 24 * 60 >= pr['nyquist_min']:\n",
    "            # Also try VSX period even if not in top peaks\n",
    "            fold_period = pr['vsx_period']\n",
    "            period_label = f\"VSX: {pr['vsx_period']*24*60:.1f} min (forced)\"\n",
    "\n",
    "        phase = (t_phase % fold_period) / fold_period\n",
    "        ax.scatter(phase, f_phase, s=1, alpha=0.3, c='navy')\n",
    "        ax.scatter(phase + 1, f_phase, s=1, alpha=0.3, c='navy')\n",
    "\n",
    "        # Binned phase curve\n",
    "        n_bins = 30\n",
    "        bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "        bin_means = []\n",
    "        for b in range(n_bins):\n",
    "            in_bin = (phase >= bin_edges[b]) & (phase < bin_edges[b+1])\n",
    "            bin_means.append(np.nanmedian(f_phase[in_bin]) if np.any(in_bin) else np.nan)\n",
    "        bin_means = np.array(bin_means)\n",
    "        ax.plot(bin_centers, bin_means, 'r-', lw=2, alpha=0.8, label='Binned median')\n",
    "        ax.plot(bin_centers + 1, bin_means, 'r-', lw=2, alpha=0.8)\n",
    "\n",
    "        ax.set_xlabel('Phase')\n",
    "        ax.set_ylabel('Norm. Flux')\n",
    "        ax.set_title(f'TIC {tic_id}\\n{period_label}', fontsize=10)\n",
    "        ax.set_xlim(0, 2)\n",
    "        ax.legend(fontsize=7)\n",
    "\n",
    "    # Hide unused\n",
    "    for jj in range(j + 1, n_phase_rows * n_phase_cols):\n",
    "        axes2[jj // n_phase_cols][jj % n_phase_cols].set_visible(False)\n",
    "\n",
    "    plt.suptitle('Phase-Folded Light Curves (Quiescent Only, FAP < 0.01)', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('phase_folded_top.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"✓ Saved phase-folded plots to phase_folded_top.png\")\n",
    "else:\n",
    "    print(\"\\nNo candidates with significant periods (FAP < 0.01) in quiescent data.\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PERIOD & VARIABILITY SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "for tic_id, pr in period_results.items():\n",
    "    flux = lc_data[tic_id]['flux']\n",
    "    std = np.nanstd(flux)\n",
    "    med_f = np.nanmedian(flux)\n",
    "    n3sig = np.sum((flux > med_f + 3*std) | (flux < med_f - 3*std))\n",
    "    n5sig = np.sum((flux > med_f + 5*std) | (flux < med_f - 5*std))\n",
    "    t_loc = lc_data[tic_id]['time']\n",
    "    baseline = t_loc[-1] - t_loc[0]\n",
    "\n",
    "    sig_str = \"SIGNIFICANT\" if pr['fap'] < 0.01 else \"not significant\"\n",
    "    print(f\"\\n  TIC {tic_id} (Gaia DR3 {pr['gaia_id']}):\")\n",
    "    print(f\"    Best period (quiescent): {pr['best_period_min']:.2f} min (FAP={pr['fap']:.2e}, {sig_str})\")\n",
    "    print(f\"    Nyquist limit: {pr['nyquist_min']:.1f} min\")\n",
    "    print(f\"    Top 5 peaks: {', '.join(f'{p:.1f}' for p in pr['top_periods_min'])} min\")\n",
    "    if pr['vsx_period']:\n",
    "        vsx_min = pr['vsx_period']*24*60\n",
    "        match_info = f\"MATCHED ({pr['vsx_match_type']})\" if pr['vsx_match'] else \"no match\"\n",
    "        if vsx_min < pr['nyquist_min']:\n",
    "            match_info = f\"BELOW NYQUIST ({vsx_min:.0f} < {pr['nyquist_min']:.0f} min)\"\n",
    "        print(f\"    VSX period: {vsx_min:.1f} min - {match_info}\")\n",
    "    print(f\"    Data: {pr['n_quiescent']} quiescent + {pr['n_outburst']} outburst pts, {baseline:.1f} day baseline\")\n",
    "    if baseline > 0 and n3sig > 0:\n",
    "        print(f\"    Outliers: {n3sig} (>3sig), {n5sig} (>5sig) | Flare rate: ~{n3sig/baseline:.1f}/day\")\n",
    "\n",
    "print(f\"\\n  NOTE: TESS FFI cadence (~30 min) limits orbital period detection to >{60:.0f} min.\")\n",
    "print(f\"  For CVs with shorter orbital periods, use:\")\n",
    "print(f\"    - TESS 2-min short-cadence data (if available)\")\n",
    "print(f\"    - ZTF forced photometry via IRSA\")\n",
    "print(f\"    - Ground-based fast photometry (time-resolved CCD)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 19: Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: Next steps summary\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "next_steps = \"\"\"\n",
    "# Next Steps for Candidate Follow-up\n",
    "\n",
    "## Automated pipeline completed in this notebook:\n",
    "\n",
    "### Cross-matching (Cells 7-13)\n",
    "- **VSX**: Variable star classifications and known periods\n",
    "- **ROSAT**: X-ray detections (accretion indicator)\n",
    "- **TESS**: Sector coverage check\n",
    "- **SIMBAD**: Existing astronomical classifications\n",
    "- **Gaia DR3 vari_classifier_result**: Automated variability classes\n",
    "- **ZTF**: Optical light curve coverage and observation counts\n",
    "- **GALEX**: UV excess from hot accretion disks (FUV/NUV)\n",
    "\n",
    "### Analysis (Cells 14-18)\n",
    "- **Multi-wavelength priority scoring**: Composite ranking from all cross-matches\n",
    "- **TESS light curve extraction**: Top 10 candidates with aperture photometry\n",
    "- **Deep candidate investigation**: Extended Gaia params (Teff, log(g), RUWE), XMM-Newton, eROSITA, SDSS spectroscopy, physical interpretation\n",
    "- **Period analysis**: Lomb-Scargle periodograms + phase folding for significant periods\n",
    "\n",
    "## Remaining manual follow-up:\n",
    "\n",
    "### 1. For STRONG CV candidates\n",
    "- **ZTF forced photometry**: Request via IRSA for dense multi-year light curves\n",
    "- **TESS extended sectors**: Extract all available sectors (this pipeline extracts first sector only)\n",
    "- **Spectroscopy**: Check SDSS/LAMOST archives or request new observations\n",
    "  - Key diagnostics: H-alpha emission, He II 4686, Balmer decrement\n",
    "\n",
    "### 2. Period refinement\n",
    "- Cross-check Lomb-Scargle periods against ZTF forced photometry\n",
    "- For eclipsing systems: measure eclipse timing for O-C diagrams\n",
    "- For superhumpers: look for period excess over orbital period\n",
    "\n",
    "### 3. Classification confirmation\n",
    "- Compare with Ritter & Kolb CV catalog (Edition 7.24+)\n",
    "- Check against AAVSO International Database for historical outbursts\n",
    "- Verify against Downes et al. CV atlas\n",
    "\n",
    "### 4. Publication path\n",
    "- **Quick announcement**: RNAAS (1-2 weeks turnaround)\n",
    "- **Full characterization**: JAAVSO, A&A, or PASP\n",
    "- **Multiple new candidates**: Batch paper in MNRAS\n",
    "\n",
    "## Files generated\n",
    "- `cv_candidates_YYYYMMDD_HHMM.csv` - Full candidate table with all cross-matches\n",
    "- `cv_candidates_high_priority_YYYYMMDD_HHMM.csv` - High-priority subset\n",
    "- `anomaly_detection_v2.png` - ML diagnostic plots\n",
    "- `tess_lc_top10.png` - TESS light curves for top 10\n",
    "- `periodograms_top5.png` - Lomb-Scargle periodograms\n",
    "- `phase_folded_top.png` - Phase-folded light curves (if significant periods found)\n",
    "\n",
    "## Re-run with different parameters\n",
    "- Adjust `bp_rp_max` to target bluer/redder sources\n",
    "- Increase `sample_size` for more candidates\n",
    "- Focus on specific sky regions (add RA/Dec filters to Gaia query)\n",
    "- Adjust `n_lc_extract` to extract more/fewer TESS light curves\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(next_steps))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28wrh2c8wvx",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update CONFIG in the running kernel\n",
    "CONFIG['sample_size'] = 5000\n",
    "CONFIG['anomaly_score_min'] = 0.3\n",
    "CONFIG['require_consensus'] = False\n",
    "\n",
    "print(\"CONFIG updated in kernel:\")\n",
    "print(f\"  sample_size: {CONFIG['sample_size']}\")\n",
    "print(f\"  anomaly_score_min: {CONFIG['anomaly_score_min']}\")\n",
    "print(f\"  require_consensus: {CONFIG['require_consensus']}\")\n",
    "print(f\"  priority_score_min: {CONFIG['priority_score_min']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
